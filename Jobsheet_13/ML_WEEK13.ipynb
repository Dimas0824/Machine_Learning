{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9AeBe5MlgI/sEHv2Kwc+j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimas0824/Machine_Learning/blob/main/Jobsheet_13/ML_WEEK13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Praktikum 1"
      ],
      "metadata": {
        "id": "jO5vyf5wjWFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nwTgfVtLi89R",
        "outputId": "e29e8f2c-a787-4766-b7e9-bbc5fed37daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25491446917804494\n",
            "Epoch 1000, Loss: 0.20602602264180095\n",
            "Epoch 2000, Loss: 0.16742418599311107\n",
            "Epoch 3000, Loss: 0.09097069374601213\n",
            "Epoch 4000, Loss: 0.025433652379636476\n",
            "Epoch 5000, Loss: 0.0115355492288669\n",
            "Epoch 6000, Loss: 0.007023129122780075\n",
            "Epoch 7000, Loss: 0.0049360862072992395\n",
            "Epoch 8000, Loss: 0.003763875630922241\n",
            "Epoch 9000, Loss: 0.0030227066091228646\n",
            "Prediksi:\n",
            "[[0.04274377]\n",
            " [0.95222173]\n",
            " [0.95245898]\n",
            " [0.06077781]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tugas 1:\n",
        "\n",
        "* Ubah jumlah neuron hidden layer menjadi 3.\n",
        "\n",
        "* Bandingkan hasil loss dengan konfigurasi awal."
      ],
      "metadata": {
        "id": "p2v9vxlGj3dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hd4KqQ1JkBgn",
        "outputId": "3efa2bab-542e-44ca-c765-01a064ae5203"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.267429293447181\n",
            "Epoch 1000, Loss: 0.2335236255859649\n",
            "Epoch 2000, Loss: 0.07240969364074616\n",
            "Epoch 3000, Loss: 0.017021822040387657\n",
            "Epoch 4000, Loss: 0.008065094005131578\n",
            "Epoch 5000, Loss: 0.005035932533295781\n",
            "Epoch 6000, Loss: 0.0035885835046952815\n",
            "Epoch 7000, Loss: 0.002758691713314847\n",
            "Epoch 8000, Loss: 0.002226763684095194\n",
            "Epoch 9000, Loss: 0.001859330592831659\n",
            "Prediksi:\n",
            "[[0.04428184]\n",
            " [0.96339008]\n",
            " [0.95782466]\n",
            " [0.03587744]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dari hasil diatas, model dengan menggunakan 2 hidden layer lebih stabil diawal tapi konvergensi lebih lambat daripada model 3 hidden layer. Sedangkan model dengan 3 hidden layer memiliki loss awal lebih tinggi daripada model 2 hidden layer, tapi setelah 2000 epoch performa model jauh lebih baik dan konvergen lebih cepat."
      ],
      "metadata": {
        "id": "Ptc_c2I8kKgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
      ],
      "metadata": {
        "id": "R-AaskUzkIq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)   # gunakan ReLU di hidden layer\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)  # output tetap sigmoid\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "# Output akhir lebih rapi\n",
        "print(\"\\nPrediksi akhir:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]} -> Target: {y[i][0]} | Prediksi: {a2[i][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HCkxQr_Ik8dD",
        "outputId": "12b778d3-c448-4a4d-bfc0-4052357cbd84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.223214\n",
            "Epoch 1000, Loss: 0.126319\n",
            "Epoch 2000, Loss: 0.125457\n",
            "Epoch 3000, Loss: 0.125249\n",
            "Epoch 4000, Loss: 0.125171\n",
            "Epoch 5000, Loss: 0.125128\n",
            "Epoch 6000, Loss: 0.125107\n",
            "Epoch 7000, Loss: 0.125084\n",
            "Epoch 8000, Loss: 0.125072\n",
            "Epoch 9000, Loss: 0.125062\n",
            "\n",
            "Prediksi akhir:\n",
            "Input: [0 0] -> Target: 0 | Prediksi: 0.4999\n",
            "Input: [0 1] -> Target: 1 | Prediksi: 0.9906\n",
            "Input: [1 0] -> Target: 1 | Prediksi: 0.4999\n",
            "Input: [1 1] -> Target: 0 | Prediksi: 0.0121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model dengan tambahan activation ReLU, memiliki loss tidak turun mendekati nol, melainkan stagnan dan pletau di sekitar 0.125. Ini menunjukkan model tidak dapat belajar pola data XOR. Pada model tanpa ReLU, prediksi mendektai target dengan baik. Sedangkan dengan ReLU activation model juga hampir benar tapi memiliki kesalahan yang lebih tinggi daripada model sebelumnya."
      ],
      "metadata": {
        "id": "SFFEdmcFlN0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Praktikum 2\n",
        "\n",
        "Pada praktikum ini kita akan menggunakan library Keras untuk menggunakan JST. Keras adalah API tingkat tinggi untuk membangun JST dengan mudah, sedangkan TensorFlow adalah framework yang mendukung Keras.\n"
      ],
      "metadata": {
        "id": "4sn8sTzcjYp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TjwCQLoUja9X",
        "outputId": "8cae4d7a-ac58-4c37-cd02-ef049218a3b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2968 - loss: 3.9875\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3915 - loss: 2.8841 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2826 - loss: 2.6843     \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3217 - loss: 2.0982 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3365 - loss: 1.7211 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3542 - loss: 1.4065\n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2971 - loss: 1.2873 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4055 - loss: 1.1206 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3400 - loss: 1.0747 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3233 - loss: 1.0247 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3684 - loss: 0.9758 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3281 - loss: 0.9659 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3605 - loss: 0.9354 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4492 - loss: 0.9308 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5834 - loss: 0.8914 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6643 - loss: 0.8577 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6731 - loss: 0.8663 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7601 - loss: 0.8047 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6976 - loss: 0.8527 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7528 - loss: 0.7956 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8094 - loss: 0.7673 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7920 - loss: 0.7665 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9079 - loss: 0.7408 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8488 - loss: 0.7600 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8881 - loss: 0.7193 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8415 - loss: 0.6944 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8379 - loss: 0.6833 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8394 - loss: 0.7013 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8856 - loss: 0.6480 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8808 - loss: 0.6617 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8429 - loss: 0.6578 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8952 - loss: 0.6136 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8683 - loss: 0.5610 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9008 - loss: 0.6204 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8886 - loss: 0.6035 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9149 - loss: 0.5571 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8846 - loss: 0.5625 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9161 - loss: 0.5353 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9254 - loss: 0.5166 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9327 - loss: 0.5449 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9182 - loss: 0.5567 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9843 - loss: 0.4237 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9187 - loss: 0.4869 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9610 - loss: 0.5277 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9653 - loss: 0.4522 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9641 - loss: 0.4323 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.4433 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9591 - loss: 0.4191 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9732 - loss: 0.4024 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9696 - loss: 0.4163 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.9333 - loss: 0.4455\n",
            "Akurasi: 0.9333333373069763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tugas 2\n",
        "\n",
        "* Ubah jumlah neuron hidden layer.\n",
        "\n",
        "* Bandingkan akurasi dengan konfigurasi awal.\n",
        "\n",
        "jawaban:\n",
        "\n",
        "# Bangun model <br>\n",
        "model = tf.keras.Sequential([ <br>\n",
        "    tf.keras.layers.Dense(12, activation='relu', input_shape=(4,)), # Hidden <br>layer 1: 10 -> 12<br>\n",
        "    tf.keras.layers.Dense(10, activation='relu'), # Hidden layer 2: 8 -> 10<br>\n",
        "    tf.keras.layers.Dense(3, activation='softmax') # Hidden layer 3: 3 -> 3 <br>(karena kelas iris hanya 3)<br>\n",
        "])<br>\n",
        "\n",
        "lalu untuk hasil akurasinya meningkat dari yang konfigurasi awal hanya Akurasi: 0.9333333373069763, meningkat menjadi Akurasi: 0.9666666388511658. Peningkatan ini terjadi karena penambahan neuron membuat kapasitas model lebih besar dalam mempelajari dan merepresentasikan data. Dengan kompleksitas yang lebih tinggi, jaringan dapat mengenali pola yang lebih rumit serta mengekstraksi fitur dengan lebih baik. Namun, konsekuensinya adalah risiko overfitting pada data latih juga bertambah jika jumlah neuron terlalu banyak."
      ],
      "metadata": {
        "id": "B2BrUPFlpxP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(12, activation='relu', input_shape=(4,)), # Hidden layer 1: 10 -> 12\n",
        "    tf.keras.layers.Dense(10, activation='relu'), # Hidden layer 2: 8 -> 10\n",
        "    tf.keras.layers.Dense(3, activation='softmax') # Hidden layer 3: 3 -> 3 (karena kelas iris hanya 3)\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQP0uIdZp0PI",
        "outputId": "008d8d8f-a4c0-48fd-9cc2-02cca4658e77"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1399 - loss: 2.5848\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2040 - loss: 1.6672 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2114 - loss: 1.2708 \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3381 - loss: 1.1178 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3566 - loss: 1.0658 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3368 - loss: 1.0536 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2880 - loss: 1.0444     \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2913 - loss: 1.0118 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2968 - loss: 0.9791 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3645 - loss: 0.9257 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3404 - loss: 0.9093 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3521 - loss: 0.8986 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3946 - loss: 0.8656 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6455 - loss: 0.8470 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6326 - loss: 0.8550 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7180 - loss: 0.7998 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7417 - loss: 0.7642 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6840 - loss: 0.7929 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7116 - loss: 0.7387 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7334 - loss: 0.7030 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7465 - loss: 0.6976 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7107 - loss: 0.6802 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7093 - loss: 0.7078 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7987 - loss: 0.6240 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7115 - loss: 0.6293 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7628 - loss: 0.5964 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8161 - loss: 0.5667 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8123 - loss: 0.5408 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8132 - loss: 0.5819 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8598 - loss: 0.5040 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8179 - loss: 0.4996 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8565 - loss: 0.5032\n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.4924  \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8550 - loss: 0.4837 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9271 - loss: 0.4385 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8508 - loss: 0.4334 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8860 - loss: 0.4600 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9490 - loss: 0.4443 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9319 - loss: 0.3984 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9292 - loss: 0.3888 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9251 - loss: 0.3848 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9351 - loss: 0.3898 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9265 - loss: 0.3370 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9256 - loss: 0.3735 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9554 - loss: 0.3589 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9726 - loss: 0.3513 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9602 - loss: 0.3360 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9705 - loss: 0.3133 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9706 - loss: 0.3138 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9661 - loss: 0.3259 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9667 - loss: 0.4053\n",
            "Akurasi: 0.9666666388511658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tugas 3\n",
        "\n",
        "* Bandingkan Sigmoid vs ReLU pada dataset Iris.\n",
        "\n",
        "\n",
        "jawaban:\n",
        "\n",
        "ReLU activation memberikan hasil lebih baik dengan akurasi 96,6% dibandingkan dengan Sigmoid dengan akurasi hanya mencapai 93,3% meskipun dengan hidden layer yang sama. Hal ini disebabkan beberapa faktor:\n",
        "\n",
        "1. ReLU tidak mengalami vanishing gradient seperti yang dialami Sigmoid. ReLU menghasilkan output \\max (0,x), gradien tetap stabil pada nilai positif sehingga proses pembelajaran lebih cepat dan konvergensi lebih efisien.\n",
        "Sedangkan sigmoid membatasi output pada rentang (0,1), gradien mengecil untuk input besar atau negatif sehingga pembelajaran lebih lambat dan cenderung stagnan.\n",
        "2. ReLU lebih baik dalam ekstraksi pola kompleks, dimana Sigmoid menekan nilai besar sehingga membuat representasi internal terlalu linear, sedangkan ReLU mempertahankan nilai besar, sehingga model bisa belajar pola non-linear lebih baik."
      ],
      "metadata": {
        "id": "ArUDixy3p4jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(12, activation='sigmoid', input_shape=(4,)), # Hidden layer 1: 10 -> 12\n",
        "    tf.keras.layers.Dense(10, activation='sigmoid'), # Hidden layer 2: 8 -> 10\n",
        "    tf.keras.layers.Dense(3, activation='softmax') # Hidden layer 3: 3 -> 3 (karena kelas iris hanya 3)\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "chA_uwk7p6qD",
        "outputId": "0086c910-590c-4996-9747-c67441691c7c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3596 - loss: 1.1654\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3558 - loss: 1.0986 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4400 - loss: 1.0849 \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4042 - loss: 1.0868 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3636 - loss: 1.1047 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3696 - loss: 1.0868 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4249 - loss: 1.0772 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3727 - loss: 1.0834 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3409 - loss: 1.0789 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3555 - loss: 1.0667 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3318 - loss: 1.0771 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4507 - loss: 1.0531 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6534 - loss: 1.0395\n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7386 - loss: 1.0274  \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6913 - loss: 1.0261 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7089 - loss: 1.0114 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7378 - loss: 0.9932 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6907 - loss: 0.9813 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7013 - loss: 0.9625 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7105 - loss: 0.9228 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7285 - loss: 0.9201 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6744 - loss: 0.9072 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7466 - loss: 0.8761 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6661 - loss: 0.8715 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6754 - loss: 0.8499 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6649 - loss: 0.8297 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7194 - loss: 0.7966 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7157 - loss: 0.7583 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6988 - loss: 0.7277 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7781 - loss: 0.7029 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7585 - loss: 0.6902 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7238 - loss: 0.6980 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7624 - loss: 0.7137 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9394 - loss: 0.6595 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8525 - loss: 0.6695 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8222 - loss: 0.6378 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9027 - loss: 0.5801 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8861 - loss: 0.5914 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8823 - loss: 0.5921 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8763 - loss: 0.5803 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8574 - loss: 0.5653 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9255 - loss: 0.5278 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8937 - loss: 0.5673 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8140 - loss: 0.5646 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9214 - loss: 0.5443 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9194 - loss: 0.5068 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9370 - loss: 0.5138 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9619 - loss: 0.5298 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8964 - loss: 0.5262 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9266 - loss: 0.5270 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.9333 - loss: 0.5105\n",
            "Akurasi: 0.9333333373069763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Catat perbedaan loss dan akurasi.\n",
        "\n",
        "    | Model              | Akurasi Akhir | Loss Akhir |\n",
        "    |--------------------|---------------|------------|\n",
        "    | **Model ReLU**     | **0.9667**    | **0.4053** |\n",
        "    | **Model Sigmoid**  | **0.9333**    | **0.5105** |\n"
      ],
      "metadata": {
        "id": "zpVEcjtBuv5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Praktikum 3"
      ],
      "metadata": {
        "id": "eauQWCwujbdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Contoh dataset (buat dummy data)\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Evaluasi\n",
        "print(\"Prediksi:\", model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bTQqy3Jqjcwy",
        "outputId": "283a8bb4-ee9d-457b-c27f-fa18cb93ba12"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.6336\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.6168\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.6001\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.5835\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.5671\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.5508\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.5345\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.5185\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.5025\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.4866\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.4709\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.4553\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.4398\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.4245\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 1.4093\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.3942\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.3792\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 1.3644\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 1.3497\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.3351\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 1.3207\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 1.3064\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.2922\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.2781\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.2642\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 1.2504\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.2367\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.2232\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.2098\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.1965\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.1834\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 1.1704\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1575\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 1.1447\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1.1320\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.1195\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1071\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0948\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0826\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0706\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.0586\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0468\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0351\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0236\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0121\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0008\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9895\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.9784\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.9674\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9566\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9458\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9352\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.9246\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9142\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9039\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.8937\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.8836\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8736\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.8637\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.8539\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.8442\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8347\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8252\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.8158\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.8066\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.7974\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.7884\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.7794\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.7706\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.7618\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.7531\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.7446\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.7361\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.7278\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.7195\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.7113\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.7032\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.6953\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6874\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.6796\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.6719\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.6643\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6567\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6493\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.6420\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.6347\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.6276\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6205\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.6136\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.6067\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.5999\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.5931\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.5865\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.5799\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.5735\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.5671\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.5608\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.5546\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.5485\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.5424\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Prediksi: [[-0.2979141]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tugas 4\n",
        "\n",
        "* Ubah learning rate.\n",
        "\n",
        "* Bandingkan hasil loss.\n",
        "\n",
        "jawaban:\n",
        "\n",
        "Pada kode pertama, model menggunakan optimizer Adam dengan learning rate default (0.001). Sedangkan pada kode kedua, learning rate diturunkan menjadi 0.0005. Perbedaan nilai learning rate ini berdampak langsung pada pola konvergensi saat proses training.\n",
        "\n",
        "Model dengan learning rate default cenderung melakukan pembaruan bobot yang lebih agresif. Hal ini membuat loss memang turun stabil, tetapi titik awalnya lebih tinggi dan proses konvergensinya sedikit lebih \"kasar\". Akibatnya, meskipun tetap membaik, kualitas akhir model tidak seoptimal yang diharapkan.\n",
        "\n",
        "Sebaliknya, model dengan learning rate lebih kecil (0.0005) menunjukkan perilaku belajar yang lebih halus dan terkendali. Penurunan loss berlangsung dengan stabil tanpa overshoot, sehingga model mampu mencapai nilai loss akhir yang jauh lebih rendah. Ini menandakan bahwa model lebih presisi dalam menyesuaikan bobot dan menemukan pola pada data.\n",
        "\n",
        "  | Model                          | Learning Rate | Loss Epoch 1 | Loss Epoch Terakhir | Catatan                           |\n",
        "  |--------------------------------|---------------|--------------|----------------------|-----------------------------------|\n",
        "  | Adam (Default LR)              | 0.001         | 1.6336       | 0.5424 (epoch 100)   | Turun stabil, tapi start lebih tinggi |\n",
        "  | Adam (Custom LR = 0.0005)      | 0.0005        | 0.7771       | ~0.2811 (epoch 72)   | Loss lebih kecil dan lebih stabil |\n"
      ],
      "metadata": {
        "id": "FYovH1mhvP9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Contoh dataset (buat dummy data)\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Evaluasi\n",
        "print(\"Prediksi:\", model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "akC_oX_ZvWLb",
        "outputId": "1261ae5e-e2eb-4c57-b258-49199baaa729"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 748ms/step - loss: 0.7771\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.7677\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.7584\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.7491\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.7398\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.7307\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.7216\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.7125\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.7035\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.6946\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.6857\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.6769\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.6682\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.6595\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.6509\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6424\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6339\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.6256\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.6172\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.6090\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.6008\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.5927\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.5847\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.5767\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.5688\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.5610\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.5532\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5455\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.5379\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.5304\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.5229\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5156\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5082\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.5010\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.4938\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4867\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4797\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.4727\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.4658\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.4590\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.4523\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.4456\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.4390\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.4325\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.4260\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.4197\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.4136\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.4075\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.4016\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.3957\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.3898\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.3840\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.3783\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.3727\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3671\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.3616\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3561\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.3507\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3453\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.3400\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3348\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.3296\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3245\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.3195\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.3145\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3095\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3046\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.2998\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2951\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.2903\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.2857\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.2811\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2765\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2721\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2676\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.2632\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2589\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2546\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.2504\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2463\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.2421\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.2381\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2341\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.2301\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2262\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2223\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2185\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2148\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2111\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.2074\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2038\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2002\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.1967\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.1933\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1899\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1865\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.1832\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.1799\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1767\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1735\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Prediksi: [[0.6279528]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Praktikum berikut akan menggunakan data Boston untuk memprediksi harga rumah.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nXJENy1ow-AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP regresi (Keras)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing # Mengubah load_boston ke fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Load\n",
        "# data = load_boston() # Baris ini dihapus karena dalam scikit 1.2 data sudah dihapus karena masalah ethical racists\n",
        "# Menggunakan California housing dataset sebagai alternatif\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# 2. Preprocess\n",
        "scaler = StandardScaler()\n",
        "Xs = scaler.fit_transform(X)\n",
        "X_train, X_val, y_train, y_val = train_test_split(Xs, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Build model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mae'])\n",
        "\n",
        "# 4. Train\n",
        "h = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=32, verbose=0)\n",
        "\n",
        "# 5. Plot\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1); plt.plot(h.history['loss'], label='train_loss'); plt.plot(h.history['val_loss'], label='val_loss'); plt.legend(); plt.title('MSE')\n",
        "plt.subplot(1,2,2); plt.plot(h.history['mae'], label='train_mae'); plt.plot(h.history['val_mae'], label='val_mae'); plt.legend(); plt.title('MAE')\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "pred = model.predict(X_val)\n",
        "print('RMSE:', np.sqrt(mean_squared_error(y_val, pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8gr64ap1xAFa",
        "outputId": "4bfb6a57-1cda-4112-ac5c-06d79eb96873"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAF2CAYAAACmtO2KAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAqGRJREFUeJzs3Xd8U9X7B/BPko50DzophbKHTCuUMgSlCqIIKMqSpaJfFPVrxYEDFP2BE3GgKMr6AoIoThTEShVk711oKS2F7r3TJvn9cXpzkw7oSNq0+bxfr7zSJjc3Jynce59znvMchV6v14OIiIiIiKgFUTZ1A4iIiIiIiMyNgQ4REREREbU4DHSIiIiIiKjFYaBDREREREQtDgMdIiIiIiJqcRjoEBERERFRi8NAh4iIiIiIWhwGOkRERERE1OIw0CEiIiIiohaHgQ4REREREbU4DHSIamHNmjVQKBRQKBTYs2dPlef1ej2Cg4OhUChwzz33GB4vKCjAwoUL0bNnT7i4uKBVq1bo27cvnnnmGVy7ds2w3euvv27Yf3W3lJSURvmcRETUPNT3vCTJycmBWq2GQqHAuXPnqn2PmTNn1nheUqvVZv9MROZm19QNIGpO1Go1Nm7ciCFDhpg8/vfffyMpKQmOjo6Gx8rKynDrrbfi/PnzmDFjBp566ikUFBTgzJkz2LhxI8aPH4/WrVub7Ofzzz+Hq6trlff19PS0yOchIqLmrS7nJWNbtmyBQqFAQEAANmzYgLfeeqva7RwdHfHVV19VeVylUjW88UQWxkCHqA5Gjx6NLVu24OOPP4adnfzfZ+PGjQgNDUVGRobhsR9//BHHjh3Dhg0bMGXKFJP9lJSUQKPRVNn/hAkT4OPjY7kPQERELUpdzkvG1q9fj9GjR6Ndu3bYuHFjjYGOnZ0dHnroIYu0ncjSmLpGVAeTJ09GZmYmdu7caXhMo9Hgu+++qxLMxMXFAQAGDx5cZT9qtRru7u6WbSwREbV4dTkvSRITE7F7925MmjQJkyZNQnx8PPbu3dtYTSZqNAx0iOogJCQE4eHh+OabbwyP/f7778jNzcWkSZNMtm3Xrh0AYN26ddDr9bXaf1ZWFjIyMkxuOTk5Zms/ERG1LHU5L0m++eYbuLi44J577sGAAQPQsWNHbNiwocb3qHxeysjIQF5entk/C5G5MdAhqqMpU6bgxx9/RHFxMQBgw4YNGDZsWJX5NuPGjUPXrl2xYMECtG/fHrNmzcKqVauQlpZW4767du0KX19fk9vAgQMt+nmIiKh5q+15SbJhwwaMHTsWTk5OAICJEyfi22+/RXl5eZVtCwsLq5yXfH198eCDD1ruAxGZCQMdojp68MEHUVxcjF9//RX5+fn49ddfq00PcHJywoEDB/D8888DEBVyHnnkEQQGBuKpp55CaWlpldd8//332Llzp8lt9erVFv9MRETUfNX2vAQAJ0+exKlTpzB58mTDY5MnT0ZGRgZ27NhRZXu1Wl3lvLRz5068/fbbFvs8RObCYgREdeTr64uIiAhs3LgRRUVF0Gq1mDBhQrXbenh44N1338W7776LhIQEREVF4f3338enn34KDw+PKpM/b731VhYjICKiOqnLeWn9+vVwcXFBhw4dEBsbC0AEMyEhIdiwYQPuvvtuk+1VKhUiIiIs/hmILIGBDlE9TJkyBbNnz0ZKSgruuuuuWpV/bteuHR5++GGMHz8eHTp0uG45TyIiorqozXlJr9fjm2++QWFhIXr06FHl+bS0NBQUFFS7zAFRc8TUNaJ6GD9+PJRKJfbv319jekBNvLy80LFjRyQnJ1uodUREZGtqc16S1tZZtGgRtmzZYnL78ssvUVRUhB9//LFxG05kQRzRIaoHV1dXfP7557h8+TLGjBlT7TYnTpxAUFBQlVS0hIQEnD17Fl27dm2MphIRkQ2ozXlJSlt7/vnnoVarqzz/3nvvYcOGDVw3h1oMBjpE9TRjxozrPr9z504sXLgQ9957LwYOHAhXV1dcunQJq1atQmlpKV5//fUqr/nuu++qTRm444474O/vb66mExFRC3S981JpaSm+//573HHHHdUGOQBw77334qOPPkJaWhr8/PwAAOXl5Vi/fn21248fPx4uLi4NbziRhTDQIbKQ+++/H/n5+fjjjz/w119/ISsrC15eXhgwYACee+453HbbbVVeM2fOnGr3tWvXLgY6RERUb9u2bUNOTk6Noz0AMGbMGHzwwQfYtGkTnn76aQAiQJo2bVq128fHxzPQIaum0Nd2JUMiIiIiIqJmgsUIiIiIiIioxWGgQ0RERERELQ4DHSIiIiIianEY6BARERERUYvDQIeIiIiIiFocBjpERERERNTiNIt1dHQ6Ha5duwY3NzcoFIqmbg4Rkc3Q6/XIz89H69atoVSyb0zC8xIRUdOp7bmpWQQ6165dQ3BwcFM3g4jIZl25cgVt2rRp6mZYDZ6XiIia3o3OTc0i0HFzcwMgPoy7u3sTt4aIyHbk5eUhODjYcBwmgeclIqKmU9tzU7MIdKS0AHd3d55QiIiaANOzTPG8RETU9G50bmLCNRERERERtTgMdIiIiIiIqMVhoENERERERC1Os5ijQ0TWS6vVoqysrKmbQfVkb28PlUrV1M0gIqo3nU4HjUbT1M0gMzLXuYmBDhHVi16vR0pKCnJycpq6KdRAnp6eCAgIYMEBImp2NBoN4uPjodPpmropZGbmODcx0CGiepGCHD8/Pzg7O/MiuRnS6/UoKipCWloaACAwMLCJW0REVHt6vR7JyclQqVQIDg7mosYthDnPTfUKdJYvX4733nsPKSkp6NOnDz755BMMGDCgxu2XLVuGzz//HImJifDx8cGECROwZMkSqNXqejeciJqOVqs1BDmtWrVq6uZQAzg5OQEA0tLS4OfnxzQ2Imo2ysvLUVRUhNatW8PZ2bmpm0NmZK5zU51D382bNyMyMhILFy7E0aNH0adPH4wcOdIQdVW2ceNGvPTSS1i4cCHOnTuHr7/+Gps3b8bLL79crwYTUdOT5uTwxNIySH9HzrUiouZEq9UCABwcHJq4JWQJ5jg31TnQWbp0KWbPno1Zs2ahR48eWLFiBZydnbFq1apqt9+7dy8GDx6MKVOmICQkBHfeeScmT56MgwcP1rvRRGQdmK7WMvDvSETNGY9hLZM5/q51CnQ0Gg2OHDmCiIgIeQdKJSIiIrBv375qXzNo0CAcOXLEENhcunQJv/32G0aPHt2AZhMREREREdWsToFORkYGtFot/P39TR739/dHSkpKta+ZMmUKFi1ahCFDhsDe3h4dO3bE8OHDr5u6Vlpairy8PJNbfR2Mz8KEz/fi5R9O1XsfRETVCQkJwbJly8yyr+joaCgUClaxsxFf7b6E+z/fi82HEpu6KUTUjJnzPNQSWbw8RXR0NBYvXozPPvsMR48exdatW7Ft2za8+eabNb5myZIl8PDwMNyCg4Pr/f45RRocTsjGueT6B0tE1HIMHz4c//3vf82yr0OHDuGxxx4zy77ItiRmFeFIQjauZhc3dVOIqJHxPNR46lR1zcfHByqVCqmpqSaPp6amIiAgoNrXvPbaa5g2bRoeffRRAECvXr1QWFiIxx57DK+88kq1pQDnz5+PyMhIw+95eXn1DnZUSpHfp9PX6+VEZGP0ej20Wi3s7G58ePT19W2EFlFLpFTw3ERE1eN5yHzqNKLj4OCA0NBQREVFGR7T6XSIiopCeHh4ta8pKiqqEsxIJeL0+uqP8I6OjnB3dze51Zd0MqnpvYjIdsycORN///03PvroIygUCigUCqxZswYKhQK///47QkND4ejoiD179iAuLg5jx46Fv78/XF1d0b9/f/z5558m+6ucMqBQKPDVV19h/PjxcHZ2RufOnfHzzz/Xu73ff/89brrpJjg6OiIkJAQffPCByfOfffYZOnfuDLVaDX9/f0yYMMHw3HfffYdevXrByckJrVq1QkREBAoLC+vdFjIv6dyk5bmJyKZY83lISqHesWMH+vXrBycnJ9x+++1IS0vD77//ju7du8Pd3R1TpkxBUVGR4XXbt2/HkCFD4OnpiVatWuGee+5BXFycyb6vXLmCBx98EJ6envD29sbYsWNx+fLlen+PtVXn1LXIyEisXLkSa9euxblz5zBnzhwUFhZi1qxZAIDp06dj/vz5hu3HjBmDzz//HJs2bUJ8fDx27tyJ1157DWPGjGmU9Rqkgg06nkyILEav16NIU94kt7p0Ynz00UcIDw/H7NmzkZycjOTkZMNo8UsvvYS3334b586dQ+/evVFQUIDRo0cjKioKx44dw6hRozBmzBgkJl5/TsUbb7yBBx98ECdPnsTo0aMxdepUZGVl1fk7PXLkCB588EFMmjQJp06dwuuvv47XXnsNa9asAQAcPnwYTz/9NBYtWoSYmBhs374dt956KwAgOTkZkydPxsMPP4xz584hOjoa9913Hzt8rIiq4uyr45AOkdk0h3NRczgPvf766/j000+xd+9eQ4CybNkybNy4Edu2bcMff/yBTz75xLB9YWEhIiMjcfjwYURFRUGpVGL8+PHQ6XQARHnokSNHws3NDbt378a///4LV1dXjBo1ChqNptbtqo86Lxg6ceJEpKenY8GCBUhJSUHfvn2xfft2Q4GCxMREkxGcV199FQqFAq+++iquXr0KX19fjBkzBv/3f/9nvk9xHYZeM12jvB2RTSou06LHgh1N8t5nF42Es0PtDmUeHh5wcHCAs7OzId32/PnzAIBFixbhjjvuMGzr7e2NPn36GH5/88038cMPP+Dnn3/G3Llza3yPmTNnYvLkyQCAxYsX4+OPP8bBgwcxatSoOn2upUuXYsSIEXjttdcAAF26dMHZs2fx3nvvYebMmUhMTISLiwvuueceuLm5oV27dujXrx8AEeiUl5fjvvvuQ7t27QCItGGyHkqldG5ioENkLs3hXNQczkNvvfUWBg8eDAB45JFHMH/+fMTFxaFDhw4AgAkTJmDXrl148cUXAQD333+/yetXrVoFX19fnD17Fj179sTmzZuh0+nw1VdfGUpGr169Gp6enoiOjsadd95Zq3bVR72KEcydOxcJCQkoLS3FgQMHEBYWZnguOjra0OMIAHZ2dli4cCFiY2NRXFyMxMRELF++HJ6eng1te61Ic3TYk0lE13PLLbeY/F5QUIB58+ahe/fu8PT0hKurK86dO3fDnrTevXsbfnZxcYG7u3uNCypfz7lz5wwnGsngwYNx8eJFaLVa3HHHHWjXrh06dOiAadOmYcOGDYZUgj59+mDEiBHo1asXHnjgAaxcuRLZ2dl1bgNZjoqpa0RUibWch4xf7+/vD2dnZ0OQIz1mvL+LFy9i8uTJ6NChA9zd3RESEgIAhnaeOHECsbGxcHNzg6urK1xdXeHt7Y2SkpIqKW7mVucRneaGqWtEludkr8LZRSOb7L3NwcXFxeT3efPmYefOnXj//ffRqVMnODk5YcKECTccZre3tzf5XaFQGIbvzcnNzQ1Hjx5FdHQ0/vjjDyxYsACvv/46Dh06BE9PT+zcuRN79+41pBi88sorOHDgANq3b2/2tlDdGQrlcESHyGya+7nIWs5Dxq9XKBQ33N+YMWPQrl07rFy5Eq1bt4ZOp0PPnj0N7SwoKEBoaCg2bNhQ5b0sXUyhxQc6cuoaTyZElqJQKGqdPtbUHBwcoNVqb7jdv//+i5kzZ2L8+PEAxIG6MSZOSrp3745///23Spu6dOlimN9oZ2eHiIgIREREYOHChfD09MRff/2F++67DwqFAoMHD8bgwYOxYMECtGvXDj/88INJRUtqOixGQGR+zeVc1FzOQ7WRmZmJmJgYrFy5EkOHDgUA7Nmzx2Sbm2++GZs3b4afn1+DCozVh8XX0WlqcupaEzeEiKxCSEgIDhw4gMuXLyMjI6PGXq7OnTtj69atOH78OE6cOIEpU6ZYZGSmJs899xyioqLw5ptv4sKFC1i7di0+/fRTzJs3DwDw66+/4uOPP8bx48eRkJCAdevWQafToWvXrjhw4AAWL16Mw4cPIzExEVu3bkV6ejq6d+/eaO2n62N5aSLb1VzOQ7Xh5eWFVq1a4csvv0RsbCz++uuvKh1qU6dOhY+PD8aOHYvdu3cjPj4e0dHRePrpp5GUlGTR9rX4QEfJ1DUiMjJv3jyoVCr06NEDvr6+NeY6L126FF5eXhg0aBDGjBmDkSNH4uabb260dt5888349ttvsWnTJvTs2RMLFizAokWLMHPmTACAp6cntm7dittvvx3du3fHihUr8M033+Cmm26Cu7s7/vnnH4wePRpdunTBq6++ig8++AB33XVXo7Xf3JYvX46QkBCo1WqEhYXh4MGD190+JycHTz75JAIDA+Ho6IguXbrgt99+a9A+zYlV14hsV3M5D9WGUqnEpk2bcOTIEfTs2RPPPvss3nvvPZNtnJ2d8c8//6Bt27a477770L17dzzyyCMoKSmx+AiPQt8MZunn5eXBw8MDubm5df5CjiZm477P9iLY2wm7X7jdQi0ksi0lJSWIj49H+/btoVarm7o51EDX+3s25PhrLps3b8b06dOxYsUKhIWFYdmyZdiyZQtiYmLg5+dXZXuNRoPBgwfDz88PL7/8MoKCgpCQkABPT09DBaO67rOyhn4vn0XH4t3tMXggtA3ee6DPjV9ARFXwXNSymePcZAMjOtKEzyZuCBER1cvSpUsxe/ZszJo1Cz169MCKFSvg7OyMVatWVbv9qlWrkJWVhR9//BGDBw9GSEgIhg0bZlKmta77NDdWXSMisrwWH+hIJ5NmMHBFRC3Yf/7zH0NZzcq3//znP03dPKul0Whw5MgRREREGB5TKpWIiIjAvn37qn3Nzz//jPDwcDz55JPw9/dHz549sXjxYsPk3/rss7S0FHl5eSa3hmDVNSJqbLZ4HrL+0hQNJJeXbtp2EJFtW7RokaGQQGVNlRLWHGRkZECr1RoWpZb4+/sbFtmr7NKlS/jrr78wdepU/Pbbb4iNjcUTTzyBsrIyLFy4sF77XLJkCd544w3zfCgYV10z2y6JiK7LFs9DLT7QYQlPIrIGfn5+tZr7QQ2n0+ng5+eHL7/8EiqVCqGhobh69Sree+89LFy4sF77nD9/vkkloby8PAQHB9e7jYYRHZ6biKiR2OJ5qMUHOnJ5aZ5MiIiaGx8fH6hUKqSmppo8npqaioCAgGpfExgYCHt7e8N6Q4BYlyglJQUajaZe+3R0dISjo2MDP43MUBGU6QZERBbT4ufoKJm6RkTUbDk4OCA0NBRRUVGGx3Q6HaKiohAeHl7tawYPHozY2FiT9SYuXLiAwMBAODg41Guf5qZUcjFrIiJLa/GBjkLBkwkRUXMWGRmJlStXYu3atTh37hzmzJmDwsJCzJo1CwAwffp0zJ8/37D9nDlzkJWVhWeeeQYXLlzAtm3bsHjxYjz55JO13qelqRRMXSMisjSbSV3jyYSIqHmaOHEi0tPTsWDBAqSkpKBv377Yvn27oZhAYmIilEq53y44OBg7duzAs88+i969eyMoKAjPPPMMXnzxxVrv09I4okNEZHktPtCRUtcY5xARNV9z587F3Llzq30uOjq6ymPh4eHYv39/vfdpaSpWXSMisrgWn7qmZOoaEZlRSEgIli1bVqttFQoFfvzxR4u2h5onrqNDRA1Rl3ORLWvxgY68jg5PJkREZB2UTKsmIrK4Fh/oyOWlm7ghREREFaS0amYbEBFZTosPdJSsbENEFb788ku0bt3apOwwAIwdOxYPP/ww4uLiMHbsWPj7+8PV1RX9+/fHn3/+abb3P3XqFG6//XY4OTmhVatWeOyxx1BQUGB4Pjo6GgMGDICLiws8PT0xePBgJCQkAABOnDiB2267DW5ubnB3d0doaCgOHz5strZR42LVNSLb1djnIoVCgS+++AL33HMPnJ2d0b17d+zbtw+xsbEYPnw4XFxcMGjQIMTFxRleU5s2lJaWYt68eQgKCoKLiwvCwsKqnTPZlFp8oCOlrml5MiGyHL0e0BQ2za0O/7cfeOABZGZmYteuXYbHsrKysH37dkydOhUFBQUYPXo0oqKicOzYMYwaNQpjxoxBYmJig7+iwsJCjBw5El5eXjh06BC2bNmCP//80zAZvry8HOPGjcOwYcNw8uRJ7Nu3D4899pihRP7UqVPRpk0bHDp0CEeOHMFLL70Ee3v7BreLmgarrhFZAM9FNXrzzTcxffp0HD9+HN26dcOUKVPw+OOPY/78+Th8+DD0er1JcZbatGHu3LnYt28fNm3ahJMnT+KBBx7AqFGjcPHixXq309xafNU1qddMrwf0er3hooGIzKisCFjcumne++VrgINLrTb18vLCXXfdhY0bN2LEiBEAgO+++w4+Pj647bbboFQq0adPH8P2b775Jn744Qf8/PPPDa7OtXHjRpSUlGDdunVwcRHt/fTTTzFmzBi88847sLe3R25uLu655x507NgRANC9e3fD6xMTE/H888+jW7duAIDOnTs3qD3UtFh1jcgCeC6q0axZs/Dggw8CAF588UWEh4fjtddew8iRIwEAzzzzjMk6Yn369LluGxITE7F69WokJiaidWvxnc+bNw/bt2/H6tWrsXjx4nq109xa/IiO0iiw4aAOEU2dOhXff/89SktLAQAbNmzApEmToFQqUVBQgHnz5qF79+7w9PSEq6srzp07Z5YRnXPnzqFPnz6GIAcABg8eDJ1Oh5iYGHh7e2PmzJkYOXIkxowZg48++gjJycmGbSMjI/Hoo48iIiICb7/9tkmKATU/rLpGZNsa+1zUu3dvw8/SemG9evUyeaykpAR5eXkAcMM2nDp1ClqtFl26dIGrq6vh9vfff1vV+anFj+gYBzpavR5KcESHyOzsnUVvVlO9dx2MGTMGer0e27ZtQ//+/bF79258+OGHAERv1M6dO/H++++jU6dOcHJywoQJE6DRaCzR8ipWr16Np59+Gtu3b8fmzZvx6quvYufOnRg4cCBef/11TJkyBdu2bcPvv/+OhQsXYtOmTRg/fnyjtI3Mi6lrRBbAc1HNzTNKdZaym6p7TJo3dKM2FBQUQKVS4ciRI1CpVCbv5erqWu92mluLD3QURmNWnPRJZCEKRa2H7JuaWq3Gfffdhw0bNiA2NhZdu3bFzTffDAD4999/MXPmTEPwUFBQgMuXL5vlfbt37441a9agsLDQMKrz77//QqlUomvXrobt+vXrh379+mH+/PkIDw/Hxo0bMXDgQABAly5d0KVLFzz77LOYPHkyVq9ezUCnmVJy6QMi8+O5yGxu1IZ+/fpBq9UiLS0NQ4cObdS21UWLT11TMXWNiCqZOnUqtm3bhlWrVmHq1KmGxzt37oytW7fi+PHjOHHiBKZMmVKlKk5D3lOtVmPGjBk4ffo0du3ahaeeegrTpk2Dv78/4uPjMX/+fOzbtw8JCQn4448/cPHiRXTv3h3FxcWYO3cuoqOjkZCQgH///ReHDh0ymcNDzQurrhFRU5yLautGbejSpQumTp2K6dOnY+vWrYiPj8fBgwexZMkSbNu2rVHbej0tPtAxSV1jigARAbj99tvh7e2NmJgYTJkyxfD40qVL4eXlhUGDBmHMmDEYOXKkoYetoZydnbFjxw5kZWWhf//+mDBhAkaMGIFPP/3U8Pz58+dx//33o0uXLnjsscfw5JNP4vHHH4dKpUJmZiamT5+OLl264MEHH8Rdd92FN954wyxto8bH1DUiaopzUW3Vpg2rV6/G9OnT8dxzz6Fr164YN24cDh06hLZt2zZqW69Hoddbf3dSXl4ePDw8kJubC3d39zq9tqRMi26vbQcAnHr9TripWY6VqKFKSkoQHx+P9u3bQ61WN3VzqIGu9/dsyPG3JWvo93LochYeWLEP7X1csGvecPM3kMgG8FzUspnj3NTiR3SkyjYAwI4zIiKyBlK2AUd0iIgsp8UHOqblpXlCISLz2LBhg0lJTePbTTfd1NTNIyunYuoaEZkBz0XX1+KrrhkN6PCEQkRmc++99yIsLKza54xLdhJVh8UIiMgceC66vhYf6CgUCigUouIa4xwiMhc3Nze4ubk1dTOomVKwvDQRmQHPRdfX4lPXADl9jalrRERkDeTUtSZuCBFRC2YjgY641zLQITIrdh60DPw7Nj4p0OGIDlHD8RjWMpnj72oTgY7CkAvdxA0haiGkvN+ioqImbgmZg/R3ZD5342HVNaKGU6lUAACNRtPELSFLMMe5qcXP0QGMJn3yhEJkFiqVCp6enkhLSwMgFrtUGFU4pOZBr9ejqKgIaWlp8PT0NFw0kOUZRnR4XiKqNzs7Ozg7OyM9PR329vZQKm2i/77FM+e5ySYCHSUnfRKZXUBAAAAYgh1qvjw9PQ1/T2ocUgccU6qJ6k+hUCAwMBDx8fFISEho6uaQmZnj3GQjgQ5T14jMTTrB+Pn5oaysrKmbQ/Vkb2/PkZwmIHU8M3WNqGEcHBzQuXNnpq+1MOY6N9Ur0Fm+fDnee+89pKSkoE+fPvjkk08wYMCAarcdPnw4/v777yqPjx49Gtu2bavP29eZkpM+iSxGpVLxQpmojuRqoE3cEKIWQKlUQq1WN3UzyArVOZlx8+bNiIyMxMKFC3H06FH06dMHI0eOrDF9ZevWrUhOTjbcTp8+DZVKhQceeKDBja8tKXWNVTmIiMgaGMpL87xERGQxdQ50li5ditmzZ2PWrFno0aMHVqxYAWdnZ6xatara7b29vREQEGC47dy5E87Ozo0c6HC9AiIish6sukZEZHl1CnQ0Gg2OHDmCiIgIeQdKJSIiIrBv375a7ePrr7/GpEmT4OLiUuM2paWlyMvLM7k1hFxemicUIiJqetKIDsDKa0REllKnQCcjIwNarRb+/v4mj/v7+yMlJeWGrz948CBOnz6NRx999LrbLVmyBB4eHoZbcHBwXZpZhariUzLQISIia6AyKsfO9DUiIsto1ILjX3/9NXr16lVj4QLJ/PnzkZuba7hduXKlQe9rqLrG1DUiIrICxst9MH2NiMgy6lR1zcfHByqVCqmpqSaPp6am3rDOdWFhITZt2oRFixbd8H0cHR3h6OhYl6Zdl5Kpa0REZEWURiM6PDUREVlGnUZ0HBwcEBoaiqioKMNjOp0OUVFRCA8Pv+5rt2zZgtLSUjz00EP1a2kDKJm6RkREVsR4jg5T14iILKPO6+hERkZixowZuOWWWzBgwAAsW7YMhYWFmDVrFgBg+vTpCAoKwpIlS0xe9/XXX2PcuHFo1aqVeVpeBxzRISIia2I8osPUNSIiy6hzoDNx4kSkp6djwYIFSElJQd++fbF9+3ZDgYLExEQolaYDRTExMdizZw/++OMP87S6juRAp0nenoiIyASrrhERWV6dAx0AmDt3LubOnVvtc9HR0VUe69q1a5Mu1il1nPFkQkRE1sAozmHqGhGRhTRq1bWmouKIDhERWRGFQmEIdtgJR0RkGTYR6HCODhFR87Z8+XKEhIRArVYjLCwMBw8erHHbNWvWQKFQmNzUarXJNjNnzqyyzahRoyz9MUxI6Wsc0SEisox6pa41N4bUNZ5MiIianc2bNyMyMhIrVqxAWFgYli1bhpEjRyImJgZ+fn7Vvsbd3R0xMTGG3xVGk/8lo0aNwurVqw2/m3NZg9oQbdIz24CIyEJsYkRH6jXjyYSIqPlZunQpZs+ejVmzZqFHjx5YsWIFnJ2dsWrVqhpfo1AoEBAQYLhJBXOMOTo6mmzj5eVlyY9RhSGtmicnIiKLsIlAR8mTCRFRs6TRaHDkyBFEREQYHlMqlYiIiMC+fftqfF1BQQHatWuH4OBgjB07FmfOnKmyTXR0NPz8/NC1a1fMmTMHmZmZNe6vtLQUeXl5JreGMqSu8dxERGQRNhLoiHumrhERNS8ZGRnQarVVRmT8/f2RkpJS7Wu6du2KVatW4aeffsL69euh0+kwaNAgJCUlGbYZNWoU1q1bh6ioKLzzzjv4+++/cdddd0Gr1Va7zyVLlsDDw8NwCw4ObvBnk85NnKNDRGQZNjFHR8nUNSIimxEeHo7w8HDD74MGDUL37t3xxRdf4M033wQATJo0yfB8r1690Lt3b3Ts2BHR0dEYMWJElX3Onz8fkZGRht/z8vIaHOwY0qp5ciIisggbGdFhegARUXPk4+MDlUqF1NRUk8dTU1MREBBQq33Y29ujX79+iI2NrXGbDh06wMfHp8ZtHB0d4e7ubnJrKFZdIyKyLBsJdMR9Uy5aSkREdefg4IDQ0FBERUUZHtPpdIiKijIZtbkerVaLU6dOITAwsMZtkpKSkJmZed1tzI2dcERElmUTgY6CC4YSETVbkZGRWLlyJdauXYtz585hzpw5KCwsxKxZswAA06dPx/z58w3bL1q0CH/88QcuXbqEo0eP4qGHHkJCQgIeffRRAKJQwfPPP4/9+/fj8uXLiIqKwtixY9GpUyeMHDmy0T6XFOiwD46IyDJsYo6OiguGEhE1WxMnTkR6ejoWLFiAlJQU9O3bF9u3bzcUKEhMTIRSKffbZWdnY/bs2UhJSYGXlxdCQ0Oxd+9e9OjRAwCgUqlw8uRJrF27Fjk5OWjdujXuvPNOvPnmm426lg6rrhERWZZNBDrS+Y+BDhFR8zR37lzMnTu32ueio6NNfv/www/x4Ycf1rgvJycn7Nixw5zNqxfp3MQ5OkRElmETqWtKjugQEZGV4YKhRESWZVuBjq6JG0JERFRBydQ1IiKLspFAR9wzPYCIiKyFNKLDcxMRkWXYSKAjVbbhyYSIiKwDsw2IiCzLJgIdlpcmIiJrI6Wucf4oEZFl2ESgo5Iq2zDSISIiK6Fi1TUiIouyiUCHqWtERGRtWHWNiMiybCrQ4bmEiIisBauuERFZlm0EOsyDJiIiK6PiGm9ERBZlG4GOVF6avWZERGQl5BGdJm4IEVELZSOBjjRHp4kbQkREVEHqhOOIDhGRZdhUoMOTCRERWQsV06qJiCzKRgIdcc8SnkREZC2kTjimVRMRWYaNBDpMXSMiIuuiYtU1IiKLso1Ap+JTcq0CIiKyFqy6RkRkWbYR6EjpATyZEBGRlWDVNSIiy7KpQIcDOkREZC1U7IQjIrIoGwl0xL2eJxMiIrISUlo1z01ERJZhG4EOS3gSEZGVYdU1IiLLso1AR8E8aCIisi6sukZEZFk2EuiIe6YHEBGRtWDVNSIiy7KRQIcnEyIisi6sukZEZFm2EejwZEJERFaGIzpERJZVr0Bn+fLlCAkJgVqtRlhYGA4ePHjd7XNycvDkk08iMDAQjo6O6NKlC3777bd6Nbg+pNQ1nkyIiMhaKDlHh4jIouzq+oLNmzcjMjISK1asQFhYGJYtW4aRI0ciJiYGfn5+VbbXaDS444474Ofnh++++w5BQUFISEiAp6enOdpfK1LqGufoEBGRtWAnHBGRZdU50Fm6dClmz56NWbNmAQBWrFiBbdu2YdWqVXjppZeqbL9q1SpkZWVh7969sLe3BwCEhIQ0rNV1pOSibEREZGWkqms6jugQEVlEnVLXNBoNjhw5goiICHkHSiUiIiKwb9++al/z888/Izw8HE8++ST8/f3Rs2dPLF68GFqttmEtrwO5GEGjvSUREdF1sROOiMiy6jSik5GRAa1WC39/f5PH/f39cf78+Wpfc+nSJfz111+YOnUqfvvtN8TGxuKJJ55AWVkZFi5cWO1rSktLUVpaavg9Ly+vLs2sguWliYjI2qhYKIeIyKIsXnVNp9PBz88PX375JUJDQzFx4kS88sorWLFiRY2vWbJkCTw8PAy34ODgBrVBaUgPaNBuiIiIzMaQusZOOCIii6hToOPj4wOVSoXU1FSTx1NTUxEQEFDtawIDA9GlSxeoVCrDY927d0dKSgo0Gk21r5k/fz5yc3MNtytXrtSlmVUwPYCIiKyN4dzEvGoiIouoU6Dj4OCA0NBQREVFGR7T6XSIiopCeHh4ta8ZPHgwYmNjoTMaTrlw4QICAwPh4OBQ7WscHR3h7u5ucmsIVrYhIiJrI52bGOgQEVlGnVPXIiMjsXLlSqxduxbnzp3DnDlzUFhYaKjCNn36dMyfP9+w/Zw5c5CVlYVnnnkGFy5cwLZt27B48WI8+eST5vsUNyCXl260tyQiIrouKXWN80eJiCyjzuWlJ06ciPT0dCxYsAApKSno27cvtm/fbihQkJiYCKVSjp+Cg4OxY8cOPPvss+jduzeCgoLwzDPP4MUXXzTfp7gBLspGRETWhmnVRESWVedABwDmzp2LuXPnVvtcdHR0lcfCw8Oxf//++ryVWTB1jYiIrA2rrhERWZbFq65ZA6auERE1b8uXL0dISAjUajXCwsJw8ODBGrdds2YNFAqFyU2tVptso9frsWDBAgQGBsLJyQkRERG4ePGipT+GCS4YSkRkWbYR6DB1jYio2dq8eTMiIyOxcOFCHD16FH369MHIkSORlpZW42vc3d2RnJxsuCUkJJg8/+677+Ljjz/GihUrcODAAbi4uGDkyJEoKSmx9McxYOoaEZFl2Uagw9Q1IqJma+nSpZg9ezZmzZqFHj16YMWKFXB2dsaqVatqfI1CoUBAQIDhZrzQtV6vx7Jly/Dqq69i7Nix6N27N9atW4dr167hxx9/bIRPJKgqzsAc0SEisgwbCXSkRdmauCFERFQnGo0GR44cQUREhOExpVKJiIgI7Nu3r8bXFRQUoF27dggODsbYsWNx5swZw3Px8fFISUkx2aeHhwfCwsJq3GdpaSny8vJMbg0ln5t4ciIisgQbCXTEPUt4EhE1LxkZGdBqtSYjMgDg7++PlJSUal/TtWtXrFq1Cj/99BPWr18PnU6HQYMGISkpCQAMr6vLPpcsWQIPDw/DLTg4uKEfzSh1rcG7IiKiathIoMM8aCIiWxEeHo7p06ejb9++GDZsGLZu3QpfX1988cUX9d7n/PnzkZuba7hduXKlwe1kMQIiIsuyqUCH5xIioubFx8cHKpUKqampJo+npqYiICCgVvuwt7dHv379EBsbCwCG19Vln46OjnB3dze5NRQL5RARWZZtBDoVn5Kpa0REzYuDgwNCQ0MRFRVleEyn0yEqKgrh4eG12odWq8WpU6cQGBgIAGjfvj0CAgJM9pmXl4cDBw7Uep/moGK2ARGRRdVrwdDmxpC6xl4zIqJmJzIyEjNmzMAtt9yCAQMGYNmyZSgsLMSsWbMAANOnT0dQUBCWLFkCAFi0aBEGDhyITp06IScnB++99x4SEhLw6KOPAhAV2f773//irbfeQufOndG+fXu89tpraN26NcaNG9don4tV14iILMumAh1WtiEian4mTpyI9PR0LFiwACkpKejbty+2b99uKCaQmJgIpVJOUMjOzsbs2bORkpICLy8vhIaGYu/evejRo4dhmxdeeAGFhYV47LHHkJOTgyFDhmD79u1VFha1JM4fJSKyLIW+GeRz5eXlwcPDA7m5ufXKi952MhlPbjyKAe298e3jjZeWQETU3DX0+NtSmeN7+f5IEp7bcgK3dvHFuocHmLmFREQtV22PwTYxR4fpAUREZG1YdY2IyLJsItBRMHWNiIisDKuuERFZlk0EOiwvTURE1oZV14iILMtGAh1x3wymIxERkY1gWjURkWXZRqCjZK8ZERFZF1ZdIyKyLNsIdKTUNV0TN4SIiKiCfG5ioENEZAk2EuiIexYjICIia2GousZTExGRRdhEoKNi1TUiIrIyrLpGRGRZNhHoKFh1jYiIrAw74YiILMsmAh2mrhERkbVRVpyBOaJDRGQZthHocPVpIiKyMlxHh4jIsmwj0GHqGhERWRkVO+GIiCzKRgIdcc/UNSIishYKjugQEVmUjQQ64mTCcwkREVkLeUSniRtCRNRC2USgo2IJTyIisjKsukZEZFk2EegomLpGRETWZPvL6LG2B+aofmYnHBGRhdhEoMNiBEREZFX0OqjKCuCuKGQnHBGRhdhEoGPIg+bJhIiIrIHaHQDghiKO6BARWYhNBDqsukZERFbFsSLQURQz0CEishCbCHSkEp5cq4CIiKyC0YgO++CIiCzDJgIdztEhIiKrovYAALgpiriODhGRhdhEoMMSnkREZFWk1DUwdY2IyFJsItBheWkiIrIqFalrrLpGRGQ5NhHoKJVMXSMiIiviWJG6xhEdIiKLsYlAR8ViBEREZE3UctU16HXQc1SHiMjs6hXoLF++HCEhIVCr1QgLC8PBgwdr3HbNmjVQKBQmN7VaXe8G1wfLSxMRkVWpmKMDAK4oZsYBEZEF1DnQ2bx5MyIjI7Fw4UIcPXoUffr0wciRI5GWllbja9zd3ZGcnGy4JSQkNKjRdaVg1TUiIrIm9mroVY4ARIlpdsQREZlfnQOdpUuXYvbs2Zg1axZ69OiBFStWwNnZGatWrarxNQqFAgEBAYabv79/gxpdV9KIDsD0NSIisg56LhpKRGRRdQp0NBoNjhw5goiICHkHSiUiIiKwb9++Gl9XUFCAdu3aITg4GGPHjsWZM2eu+z6lpaXIy8szuTWEyijSYa8ZERFZBaNFQ3luIiIyvzoFOhkZGdBqtVVGZPz9/ZGSklLta7p27YpVq1bhp59+wvr166HT6TBo0CAkJSXV+D5LliyBh4eH4RYcHFyXZlYhpa4BTF8jIiIr4SiXmOaIDhGR+Vm86lp4eDimT5+Ovn37YtiwYdi6dSt8fX3xxRdf1Pia+fPnIzc313C7cuVKg9pgkrrGXjMiIrIGannRUJ2uidtCRNQC2dVlYx8fH6hUKqSmppo8npqaioCAgFrtw97eHv369UNsbGyN2zg6OsLR0bEuTbsupq4REZG1URjm6BRBy3MTEZHZ1WlEx8HBAaGhoYiKijI8ptPpEBUVhfDw8FrtQ6vV4tSpUwgMDKxbSxtAydQ1IiKyNmpp0dAipq4REVlAnVPXIiMjsXLlSqxduxbnzp3DnDlzUFhYiFmzZgEApk+fjvnz5xu2X7RoEf744w9cunQJR48exUMPPYSEhAQ8+uij5vsUN6Bg6hoRUbNWl/XbjG3atAkKhQLjxo0zeXzmzJlV1ngbNWqUBVpeM0VFoOOuKOaCoUREFlCn1DUAmDhxItLT07FgwQKkpKSgb9++2L59u6FAQWJiIpRKOX7Kzs7G7NmzkZKSAi8vL4SGhmLv3r3o0aOH+T7FDaiMR3TYa0ZE1KxI67etWLECYWFhWLZsGUaOHImYmBj4+fnV+LrLly9j3rx5GDp0aLXPjxo1CqtXrzb8bs6U6VoxqrpWznMTEZHZ1TnQAYC5c+di7ty51T4XHR1t8vuHH36IDz/8sD5vYzZMXSMiar6M128DgBUrVmDbtm1YtWoVXnrppWpfo9VqMXXqVLzxxhvYvXs3cnJyqmzj6OhY6/mlFmFUdS2/pLzp2kFE1EJZvOqaNWDqGhFR81Tf9dsWLVoEPz8/PPLIIzVuEx0dDT8/P3Tt2hVz5sxBZmamWdt+Q0ZV17IKNY373kRENqBeIzrNjci/BvR6pq4RETUn11u/7fz589W+Zs+ePfj6669x/PjxGvc7atQo3HfffWjfvj3i4uLw8ssv46677sK+ffugUqmqbF9aWorS0lLD7w1dyBqAYUTHTVGEtCIGOkRE5mYTgQ4g5umU6/VMXSMiasHy8/Mxbdo0rFy5Ej4+PjVuN2nSJMPPvXr1Qu/evdGxY0dER0djxIgRVbZfsmQJ3njjDfM21qjq2nkGOkREZmcTqWuAPE+HqWtERM1HXddvi4uLw+XLlzFmzBjY2dnBzs4O69atw88//ww7OzvExcVV+z4dOnSAj49PjWu8mXshawBy6pqiGNlMXSMiMjubGdGR5ulwrQIioubDeP02qUS0tH5bdUVxunXrhlOnTpk89uqrryI/Px8fffQRgoODq32fpKQkZGZm1rjGm7kXshY7lauuZRWWmXffRERkO4GOSikiHQ7oEBE1L5GRkZgxYwZuueUWDBgwAMuWLauyfltQUBCWLFkCtVqNnj17mrze09MTAAyPFxQU4I033sD999+PgIAAxMXF4YUXXkCnTp0wcuTIxvtgFalrrooS5BYWNd77EhHZCJsJdJi6RkTUPNV1/bYbUalUOHnyJNauXYucnBy0bt0ad955J958883GXUunYkQHAIrzcxrvfYmIbITNBDpS6hoDHSKi5qcu67dVtmbNGpPfnZycsGPHDjO1rAHsHFBu5wy78iLoixq5tDURkQ1gMQIiIqImUu4iCirYF6U0cUuIiFoemwl0pDk6rEVARETWQu/WGgDgUpLWxC0hImp5bCbQUTJ1jYiIrIzKMwgA4FWejtJybRO3hoioZbGZQEdRkbrG8tJERGQt7D1FuesARRZyilhimojInGwm0FEpWF6aiIisi8JDpK4FKrKQxUVDiYjMymYCHaauERGR1XEXqWsBikxkFzHQISIyp5Yf6KSeAf54FQ9qfwXA1DUiIrIi7mJEJ0CRhexCpq4REZlTyw90si8Dez9BhG4PAFZdIyIiK1IxouOryENOQUETN4aIqGVp+YGOoxsAwFVfBADQM3WNiIishbM3yhQOAICy7KtN3BgiopbFBgIddwCAC4oBcESHiIisiEKBfEc/AIAul4EOEZE52UCgI0Z0XPSFADhHh4iIrEuJOgAAYJd/rYlbQkTUsrT8QEftAQBwRgmU0DF1jYiIrEqZSyAAQFWQ3MQtISJqWVp+oFMxogMArihm6hoREVkVZ1+xaCjyrkLHkxQRkdm0/EDHzhFQOQIA3FAELUd0iIjIingHhAAAgnVXcSmDldeIiMyl5Qc6AKAWBQlcFcVcMJSIiKyKKmQwdFDgVtUpXDn2Z1M3h4ioxbCNQKcifc0NRZyjQ0RE1iWgJ4773gsA6H5sEaDlwqFEROZgI4GOPKKj1TVxW4iIiCrJGPASsvSuCCi5BMT83tTNISJqEWwj0KlIXXMHU9eIiMj63NS5PX7XhgEAyhMPNnFriIhaBtsIdIxGdJi6RkRE1qa1hxpxjl0BAEWXDzVxa4iIWgabCnTcUMTy0kREZHUUCgW0Af0AAOr0U4BO28QtIiJq/mwk0KkoRqAogpaRDhERWSH/jr1RpHeEg7YQyLjY1M0hImr2bCPQkcpLc44OERFZqT5tfXBaHyJ+uXa0SdtCRNQS2EagI6WuKYrAOIeIiKxR7zYeOKnrCAAo5jwdIqIGs5FAR1pHp5ipa0REZJXc1PZIdesBANAkHm7i1hARNX+2EeiopWIETF0jIiLrpQy6GQDgkn0e0JY3cWuIiJo32wh0jFLXOKJDRETWKrhjDxTpHWGn1wDZl5u6OUREzZpNBTquKEZ2UVkTN4aIiKh6fdt6I1bfGgBQnnKmiVtDRNS81SvQWb58OUJCQqBWqxEWFoaDB2u3ivOmTZugUCgwbty4+rxt/anlEZ20/JLGfW8iIqJa6h7ojgRVWwDAtYvHmrg1RETNW50Dnc2bNyMyMhILFy7E0aNH0adPH4wcORJpaWnXfd3ly5cxb948DB06tN6NrbeKYgSuKEZafmnjvz8REVEtqJQKwLc7AKDgyqkmbg0RUfNW50Bn6dKlmD17NmbNmoUePXpgxYoVcHZ2xqpVq2p8jVarxdSpU/HGG2+gQ4cODWpwvVSkrjkqypGdm9f4709ERFRL/p36AgCcci40bUOIiJq5OgU6Go0GR44cQUREhLwDpRIRERHYt29fja9btGgR/Pz88Mgjj9TqfUpLS5GXl2dyaxAHV+ihAAAU5Wc3bF9EREQW1K13GAAgSHsVCWk5TdsYIqJmrE6BTkZGBrRaLfz9/U0e9/f3R0pKSrWv2bNnD77++musXLmy1u+zZMkSeHh4GG7BwcF1aWZVSiX0Dq4AgJL8HOhZYpqIiKyUu397FCuc4KDQ4sgxrqdDRFRfFq26lp+fj2nTpmHlypXw8fGp9evmz5+P3Nxcw+3KlSsNb0xFQQL78gIUlHJtAiIislIKBXJdOwIAHGN+AjLjmrhBRETNk11dNvbx8YFKpUJqaqrJ46mpqQgICKiyfVxcHC5fvowxY8YYHtPpdOKN7ewQExODjh07Vnmdo6MjHB0d69K0G1I6ugO4CleFKEjgprY36/6JiIjMReHXHcg/jbuz1gGfbQaePQ24+jV1s4iImpU6jeg4ODggNDQUUVFRhsd0Oh2ioqIQHh5eZftu3brh1KlTOH78uOF277334rbbbsPx48cbnpJWFxUjOu4oQloeK68RETUn5l7WQK/XY8GCBQgMDISTkxMiIiJw8eJFC7S8ftwGzkCsrjVK9XaAthRIO9vUTSIianbqnLoWGRmJlStXYu3atTh37hzmzJmDwsJCzJo1CwAwffp0zJ8/HwCgVqvRs2dPk5unpyfc3NzQs2dPODg4mPfTXE9F5TUfRS7X0iEiakYssazBu+++i48//hgrVqzAgQMH4OLigpEjR6KkxDrOD86dh+I/niuwX9dDPJBjhhRuIiIbU+dAZ+LEiXj//fexYMEC9O3bF8ePH8f27dsNBQoSExORnJxs9oY2WFAoAGCY8gTS80uBvGSgIo2OiIisl7mXNdDr9Vi2bBleffVVjB07Fr1798a6detw7do1/Pjjjxb+NLXXp40nruor5rfmMtAhIqqrehUjmDt3LhISElBaWooDBw4gLCzM8Fx0dDTWrFlT42vXrFnTNCeS7vcAAIYqT6H9hVXA0m7A/uWN3w4iIqo1SyxrEB8fj5SUFJN9enh4ICws7Lr7bGx9gz2QpPcVv+QkNm1jiIiaIYtWXbMq/j2Rpw6Ck0KDEVc+EY/t/bRp20RERNdliWUNpNfVZZ9mX9+tFnq38URSxYiOnoEOEVGd2U6go1AgLWiE6WPurZumLUREZBH1XdbgRsy+vlstdAt0Q6pSVFrTZiVY/P2IiFoa2wl0ABR3HG36QGF60zSEiIhqpSHLGtjZ2cHOzg7r1q3Dzz//DDs7O8TFxRleV9t9AhZa3+0GHO1UcPRpBwBQFiQDWq4BR0RUFzYV6Kg7DMI35bfhTwwQD+SnsCABEZEVs8SyBu3bt0dAQIDJPvPy8nDgwIFq9wmI9d3c3d1Nbo3BLzAEGr0KSr0WyLgAxO8G9PpGeW8iouauTguGNndB3i64W/84dCUaXFTPgEJXBhRnAS7mS28gIiLzioyMxIwZM3DLLbdgwIABWLZsWZVlDYKCgrBkyRLDsgbGPD09AcDk8f/+979466230LlzZ7Rv3x6vvfYaWrduXWW9nabWrbUHrp3xQYgiFdgwAci7CkzeDHQd1dRNIyKyejYV6Dg72GFkzwD8cuIaCuy84FaeBeQnM9AhIrJiEydORHp6OhYsWICUlBT07du3yrIGSmXdEhReeOEFFBYW4rHHHkNOTg6GDBmC7du3Q61WW+Ij1Fu3AHck6X0QglQR5ABA0kEGOkREtaDQ661/DDwvLw8eHh7Izc1tcLrA3tgMTPnqAH5zfAU9FPHA1O+AzneYqaVERC2LOY+/LUljfS+ZBaX48+0HMdEuWn6w+73AxP+Jn3U6QK8DVDbVb0lENq62x2CbmqMDAAM7tEJIK2ck6zzFA/lWuLgpERERgFaujsh2qFQgIeOi/POW6cAHXYCCtMZtGBFRM2BzgY5SqcCE0DZI1XuKB/JTUFhaDq3O6ge2iIjIFnm2BQBolfbi96w4QKcFykqAmN+BokwgbpfpazSFwP4VQG5SIzeWiMh62FygA4hRnTR4AQDy0hIRtjgK87acaOJWERERVVUYcgf+0fbCr23mASpHQKsBchKA9POArqLk9NXDpi86th7Y/iIQvaR+b1qSB/z8FHDp74Y1noioCdlkoNMzyAMZ8AYAXEmMQ0FpOQ7GZzVxq4iIiKpq36Y1ppfNx7KsgdB5dxAPZsQCqafljZIOmb4o7ay4z67nQqMXtgNH1wH/vFe/1xMRWQGbDHTU9io4tQoCAOhyxRydjIJSNIO6DEREZGNGdPOHj6sD4jMKcVEbKB7MuACknJI3SjkFlBXLv2fGifv6zkOV5vwUZtTv9UREVsAmAx0A8GvdXtwrsgEApeU6FGq0TdkkIiKiKjyc7fHWuF4AgJ3pHuLBzIumgY6uHEg2SsHOjBX3+anyY3G7gFWjgLRzN37TokzTeyKiZshmA532HToCAHyQCxVEgJNZUNqUTSIiIqrWqJ4BGHmTP2KlEZ10oxEdrxBxn1QxT6e0QB7J0eQDpfni52P/AxL3Aae+u/EbFlWM5BRnAcx2IKJmymYDnZ6dO6Fcr4RKoUcr5AEAMgo0TdwqIiKi6s0YFII4fWsAgD7pEFCaB6gcgD5TxAbSPJ2sONMXSqM6edcq7q/e+M2KKuat6srlQImIqJmx2UAnwNMZWUpRea2TYw4AjugQEZH1Gti+FQo9OiFN7wmFrkw86NsNaBcufr56VNxLaWsSaXRHCnBqU3LaOGWN6WtE1EzZbKADAK5B3QEAEb45AIDMQo7oEBGRdVIqFRgT2hFTNC8jQ+UrHmzdFwjsI37OTRQjMZmVR3RSAJ1O3AN1D3SKWZWUiJonmw50nIN6AgA66a8A4IgOERFZt/tvboNYfRvcXfQ6sga+CAx7EVB7AN5i3imuHasm0EkWgYu2ojMv79qN592YjOhkm+8DEBE1IpsOdOAnRnTalF8GwDk6RERk3dq2csbt3fyQqvfCstJ7AY824onWfcV98nE5dU0qUpCfAuRfk3eiLb1+2WidVp6jAzB1jYiaLRsPdHoAAPxLLgFg6hoREVm/R4eI5RG2HE5CSm4JyrU6ILCvePLacVF6GgBChoj7/GS5EIEk90rVHRdmAAe+ALIvAzAa8WHqGhE1U7Yd6Ph2BQC4lKbBHYVMXSMiIqsX3rEVugW4obhMi4FLohC2OApprt3Ekxd3AiW5gNIeaDtIPJafUjXQqa7y2p4Pgd9fAP56y/RxjugQUTNl24GO2gNwF8P+nRVJyGCgQ0REVk6hUCDyji6wUyoAiGyExcccxJPlxeI+dCbg3UH8XO2ITjWBTtpZcR//j+njRVYwoqPXi5Q6IqI6sO1ABwD8RC/YrapTCM/7Ayhn+hoREVm3O28KwJlFI/HrU0OgUirw4/lC5DkFiycdXEWRArcA8XtBqhzoKFTiPq+aymvS3J6iSvN3rCF17ZtJwCc3A2XFTd0SImpGGOhUFCR4xm4r3tB/Ct3h1U3cICIiohtztFOhZ5AHpg1sBwD4PV9UXjvedjrg6isHOmVFQPp58bP/TeK+conp8lIgp5p5O0D9Utc0hcDGiYA5zql6vUjJy75cdY0gIqLrYKBTUZBAUn7hzyZqCBERUd1F3tkFd/cOxLfej2OW5nlMOj8EsWn5gL0ToPYUG12rWEw0eIC4r5y6lhUPkwIEAOBaESjVprx0zhUgP1X+/VI0cGE78O9Hdfw01dAUAPqKtDVrSKMjomaDgU77WwFHd1yEGPJXJe0DtOVN3CgiIqLacVfbY/mUm/Hds3dB2+lOlJQDz24+Aa1OD3i2Nd24TUWgIxUj0OsBbVn1IyU+ncX9jUZ0CtKBFYOBryLEwqSAvJZPQVrtP4heDySfrJqeVpIr/8zCCERUBwx0PNoAz8fhSbePkat3hkqTD6ScaOpWERER1YlCocB7E3rDTW2HU1dz8cuJa8DAJ0w3Cu4v7vOTgbIS4PtHgLfbAjG/V92hFOjcaI7OqS0iGMlNlAORLLFsA8oKgdKC2n2AuCjgi6HA9pdMHy/OkX9moENEdcBABwDsHODl6oQDOjFfp0rFGSIiombA312N/wwTc3WW7ryAsp4PAk7e8gaeIYCLH6DXAavvAk5/L+bwHF8vnrd3kbdtVRHolJcAmqKa3/TERvlnaaQoK05+rCAVtZJ6RtynnTd9vCRH/rm4Fml0REQVGOhU8HFzxF5dxSTNfcuBt9sBR9Y2baOIiIjqaNbgEPi4OiAxqwhr9yUAM34GHNyArqMBpRK49xOxoTRvx1jH2+SfvdqJ9XiA6kdSdFqRapZySn4sP1ncZ8XLj9U2fU3arkrVtxz5Z47oEFEdMNCp0KeNhxzoFKaLHqTDq5q0TURERHXl7GCHp0eI0Zj/++0cvr3iCf2zp4GJFaM2XUcBg58RPwfdAji3kl/cdbTRjnwA54rRoD9eEaM/krTzwLvtRaqZsbxrIiXOuKpbbUd0DIFOpWDGeESHgQ4R1QEDnQoDO7TCBX0b/IRboZdWk045JXKLT24BrlbT80VERGSFHgprh4cGtoVeD7zw/UkMWnYE/ztgVD56xOvA1O+Bh74HbhovP975TnmtHRcfeUTn7E/AT3PlRTv/elMuEqBQAv69xM/5yaIMtHEFt8ojOtqy6kd5pICoONu0KJDJiE4jVF0rzQfO/SoCNiJq1hjoVOgR6A43R3s8U/IfnBm5CfAIFuUsd38AbH0UWHM3cO14UzeTiIjohpRKBRbd2xOPD+sAtb0SybkleO2nM/j15DVpA6BzBODkCfSZLB7zbCvW3xnxGhA6C/DuIBckAMRcnrxrIl3t/K8AFMAjfwIvXgZuGiu2yUuWCxFIClJMf/92BvBBN7kym2E7o+DHuABCY1dd++d9YPNU4IjRGkB6PVCSZ/n3JiKzYqBTwU6lRP/2Yoh+/6VMoO1A8cTej8V9WRHwzeSqB3AiIiIrpFQqMP+u7ji+4E48PLg9AGDelhM4fTXXdMM2twBTtgCTKooKDHkWGLMMUCiAEQuAW5+X1+PJugT88674ued9ooqb2gNway0ey79mWogAME1d0+mA+L9FR+KVAzVvV2g0T8ckda0RRnRST4v7jIvyY9ueA95pB/wa2bA2xP4p1hgyt7xkIOrNqgvBmltOohjd0+tvvG1zUZAmMnfKNU3dErIABjpGBnaQAp0sOdDRVQyfuwaIA/jng4Eja5qmgURERHWktlfhlbu7Y1gXX5SU6fDYusNIzy813ajLnUBAr6ovDroZuP1VIDhM/J5xAbjwh/h5yLPydu6B4t54RMfVX9znpwJbHwO+fxTIuSwWAAVM1+7RlpmO4hiP3BinrlVX6rogXV6/xxyyL1fs1yjwuviHqFR3+Gtgw4T67be0QHSYbpwElJfeePu6OPw1sPt9YN9n5t1vZct6Ad9OF6XAW4o/3xCZO2d/bOqWkAUw0DEysIOYkLn/UiauuvWRn/DpAsyOAkKGipGdX/4rVoEmIiJqBlRKBT6e3A8dfFxwLbcEs9cdxtWc4hu/UOLdQdxf2AFoS0UVN7+b5OeNR3SklLS24eI+6SBwcrNYb+fMD/JrjAOdwnTT9yuqYURHU2A6d+bUd8D7nYCja2r/Wa5HpxWjFoAc6Oh0pkHP1SNA7tW677sgFdBqgPJi849M5VekB2bHX3+7hjCupJdy2nLv09ikwDaX13UtUb0CneXLlyMkJARqtRphYWE4ePBgjdtu3boVt9xyCzw9PeHi4oK+ffvif//7X70bbEk9At3R2c8VBaXlGPttJrQO7uKJPpPEwqLTfwZa3wxAz7V2iIioWfFwssfKGbfATW2H41dycOfSv/HCdyew9WgSdLobpCJJgY6UdhXQU8zzkUgjOiW5QMpJ8XO7wfJjksNG816M5+hUrsxmnLpmPKIDmI7qnPxW3F/ec/3211Z+sghGjNskBSgKlRzcJdV83VMjk89k5kBHWl/Ikhfr536x3L6bkhRUV/53Zq0KM8UIKNVKnQOdzZs3IzIyEgsXLsTRo0fRp08fjBw5Emlp1dfJ9/b2xiuvvIJ9+/bh5MmTmDVrFmbNmoUdO3Y0uPHmZqdSYuPsgegV5IGMIi3Wqu6DPjgcuHmG2ECpBDoMFz8z0LGsshKRjkBERGbT0dcVPzwxGKHtvFCo0eLbw0mI/PYE5n13AtrrBTtSoKOruMCqnObm6C4vNlqcDTi4inS4yowvxDPj5JSzylXYjEc8jEd0ADmtTacFEveJn/Ouidd8eRvw70divsWae4BvptRtPonUuw+IlDu9HshJEL97BAHtKqqyXqlPoGN0TjP3iI4U6Fgy28Q40DF3oNaUpAC08r8za5R3DVjaHdg4salb0mzUOdBZunQpZs+ejVmzZqFHjx5YsWIFnJ2dsWpV9WvODB8+HOPHj0f37t3RsWNHPPPMM+jduzf27DFT74uZ+bo5Yt3DA+DmaIdF2Xfit/6rRYlNSftbxf3l3dUfPHOT5CFkqr+NDwAf9hD53kREZDad/Fyx5fFwrJp5Cx67tQNUSgW2Hr2KpzcdQ0mZFml5JUjMLDJ9kXd709/9e5r+rlAA7q3l3zveBrgHAVDU3JDyYpHqBlQd0TFJXZNGhCr2JQUJKSeB0opKaHnXgNgosQjqriViwvzl3UDMNtMRpRsxDnS0peLiN7si0PFsJ89VqlxIoTaMA536Bgr7PgN2vFL1+kP6TkpyRHlsY3nJYhSgIfKSTUexWsp6Rjqd/LdoDiM6qWfFv8ukQ03dkmajToGORqPBkSNHEBERIe9AqURERAT27dt3w9fr9XpERUUhJiYGt956a91b20i8XBzwyFBxUF+6M8a0lys4DFA5AHlXUZp2Ue5Beq+zuH14E/BRX5HDW1spp4D0GPN+iOZMrweSDotUgfRzTd0aIqIWR6lU4PZu/nh5dHd8Orkf7JQKbDuZjBEf/I2BS6Jw63u78MCKvTh+JUe8wLOtvL4OUH3hAil9DQC63AWo7E0XI1UYXXJIVdykeTqVR3SqS13zCBb30kX25X/lbQzr90AEUL8/Lz9X2wVLATmoMbw2TR7R8WoHBA8QPyefAMqM5jjlJgEnNl2/KILxZ6rPiI6mUCzcuu/TqhVgpREdqS2S0gLgszDgy2ENq5RWuVJcY1S/awwlOaLIhPSztZOC5dK8quXOrx4Bdi81XYOK6hboZGRkQKvVwt/f3+Rxf39/pKTUPIqRm5sLV1dXODg44O6778Ynn3yCO+64o8btS0tLkZeXZ3JrbA8PaQ9PZ3vEpRdi+S6jCZMOzsj0EoUKtv20Gdg+X/QgFaaJGyAOspunVb8gWkmeOBhLB5yiLODrO4Gv7xAHMRK9b2UVvYlMXyMisqi7egXif4+EwdvFAVdziqHTi+IFhy5nY+bqg6JogcpeBDtAxVyV7lV3JBUkgEIsPArIlde8QoCA3uJnJy+5sqk0T0c6X1YOZsqKRQ82ALTqYPqc8bwcrQa4dkz+3fjCv06BzmXT3/NTjEZ0QirWGgoQFVmN3++3F4AfHgdOf1/zvhs6opN8Qr4oz7smP67Xm+7PONBJPSPOqblX5NGv+sirKL7gWDF3uTYjOppC4Oj/qo4wWZPrzQWzRsYjnfmVMl5+fxGIegO4tKtx22TlGqXqmpubG44fP45Dhw7h//7v/xAZGYno6Ogat1+yZAk8PDwMt+Dg4MZopgl3tT1eu7sHAODDPy9gwud7cdOC7Yg6l4o/S7oCAO64thw4uUn0Uj34P+DRv4DI80CrzuKg8PlgMcxs3IuybizwUR/g0/5i5eUrB8VFfUkuEL+70T+nVTL+z1tY/dwvIiIyn/COrfDLU0Pwwqiu+P2Zodj70u3o3cYDOUVleHLDUZSWa+V5Oj5dAHunqjuRRnTa3CIWHgUAV7+KxwbI81v8ewKtOomfDYFORTAiBVDShbR08alQyoFWcXbF/Jy9pu9/ZX/1Hy6/AYFO5REdhUIe1TFOX7t6+PptABo+R8c4U8T4PKkplAsoAHLVOMA0K8L4or6upLb7dhP3tQl0Dn4J/DxXrO9jrYw/R3Ma0QHk4FMi/V+y9FpKzUydAh0fHx+oVCqkppoeNFJTUxEQEFDzmyiV6NSpE/r27YvnnnsOEyZMwJIlS2rcfv78+cjNzTXcrlxpmpJ/94e2waT+wdDrgcMJ2SjUaPHCdyfxXsZgxOjawA0Vw9bhTwI97gXahIoD/aSNIpe3MA3YMR848Y3YLjNOjP4AQOZFYPtLpjmvsTsb9wNaK+OequpGxYjI5pi72ufMmTOhUChMbqNGjbL0x7BqQZ5OeGJ4J3QPdIe/uxrLp9wM94oKbY+uPYwyjxCxYUDP6nfQ834RBA2dJz8mBS4dbwduni46AkNnAq06iscrp675iQ7GKhPE1R5yGlxRpjxS4eAmp9FJF62OHuJeSpUrqMg4Kc0H4v4C0iqlROu0coekFOj4dJVfazxHBzCap3NQbqsUqF07Xv13A5j2xtcr0Dkq/2wc6BiPXgGmF7pp5+WfK5fwBsTn/vFJ4Kcnr5/aJr3Wr5pAJz1GpJtXJqXkW/O1TbVzwayYcbCadw3483Xg12dFiqI0qteQgLYFqlOg4+DggNDQUERFyQtF6XQ6REVFITw8vNb70el0KC2tebEsR0dHuLu7m9yayuv33oRHhrTH0yM6I8jTCZmFGmTAA3dp3sYTmqexu+2TwO2vmb7Itwvw1BFg4BPi90Nfi/vYiu8t6BZA5SiGkk9sll93cWfLWm24vkxGdJi6RmTrLFXtc9SoUUhOTjbcvvnmm8b4OM1GsLczVkwLhbODCrsvZmDWsY6IseuKeZf64fYPojHpy33YftoobT2gFzD3ENDVKGC8/TVg1u9A74ki6HnqMNBrgjyic+WAqBRmGNGpCHSKMsX5ULr4VHuaBjpS2lrbgYBHW9OGT/wfEPE6cMsj4veCVCD6beCd9sD/xgOfDQRWDBEBTMJe4C0/YPcHYmREyiKQRm1yrwJ5FYGDNKJkXJBArxdBlyT1dM1zJBpaXvqaUaBjXKin8r6MK9uZjOhUcz4tSAWOrweOrb9+x6L0nG9F4FqcIz6nXg+sGwesGlW14psUcGVdMn81uC0zxXs2dD6KyYhOnjzHKj0GOPtzw/ZtCcZ/w9SzwJ4PgcOrTCsBV86E0RQBvzxjs9WC65y6FhkZiZUrV2Lt2rU4d+4c5syZg8LCQsyaNQsAMH36dMyfP9+w/ZIlS7Bz505cunQJ586dwwcffID//e9/eOihh8z3KSxIba/Ca/f0QOQdXfDiXd0Mj4/s2Rq/6QbivaLRgJ1j1Req7IEhkYDSXgxpp5ySezV63Au0Hyp+zjPqeclJMF1AzVYZBzoc0SGyeZaq9uno6IiAgADDzcvLqzE+TrMyqKMPNs4eCB9XR+wpDsHIgoX4LjMEl9ILsf9SFp7ceBT/XJAvvrILNaY7cHAWKWvKSpcbwQNFUFOSA6y9R744l0aAdGViTomUuubkKQc6+SlAQkUhgpDBpkUQlHZAyBBgyLNyYJKfChz4QuzTvY0oKJRyCvj7XeDACjHfZt+n8oiJ2lOMTAEiuNDrROekNN8osLf4vShTXMQbBzrlJUBGDcWFGpK6VpRVqfS1UeZDQ0Z0jBcBvd4aPNJrpe8FevG3y7sq2qIrE0GjMePUKnNeZGsKxcKzifuqFmWoK5PRDz1QWhFYb5oCfDvt+iN0jUWnFUFXUZbp3zAuqvqfK/+dz/4IHFkDbH1MlF23MXZ1fcHEiRORnp6OBQsWICUlBX379sX27dsNBQoSExOhNDqgFRYW4oknnkBSUhKcnJzQrVs3rF+/HhMnNr8a4GN6B2JfXCaKNOV4cVQ3/H46BSeTcpGWXwI/N3XVF7j6At3uFv/I9q+Q5+B0igDsnIDYP8XvDm5AUD9xILiwA/Dp3GifySrlcY4OEQlStU/jDrS6Vvv866+/EBMTg3feecfkuejoaPj5+cHLywu333473nrrLbRq1aqGPdmuvsGe2PPibbiYWoCk7CK4qu2gUiqw8UAifj2ZjCc3HMUDtwTj9LVcHIzPwvh+QXh3Qm/Yq67Tl2rnAEzdAqwcIV/AuwZUzP9xAcoKxUWoIXXNEwjsK35O3AfYO4uf2w0BLhtdRHu0AZQV1eHcKlLq087Jox5zD4qL1zWjxblZVzEiUJwN/PAf8XPnO+WgRkpP82wrB2t2jkDrfmI+zpUDpoEOIPbvf5PpYzqt6ehBXUd0jNPWANPzpBQ02TmJYkjS6Elxtpy2B1Sf0mQcPOUkiPlV1ZEunt0DRRphSa74PMYjNUkHgT4V13Z6vWkaevzfQL+pNX68OjF+z9wrIoumvirPNSrOETep0zkzFmjdt/77N4fot4F/3gVuedj0b5huFMTGGgU6lYs4SX/j/GQRIPZpftffDVHnQAcA5s6di7lz51b7XOUiA2+99Rbeeuut+ryN1VEoFFhyn1xSs2eQO05fzcO0rw7ikyn90MXfDQCQmleCd7fH4J4+gbgtdKY4mB5fL17kFih6seydgd8rdtQmFOg6WgQ6ez8Bbp4mDiS1VZQlDtbSpM/a0OvFrXIvmzUwGdFh6hqRLbtetc/z58/X8CpR7TMoKAilpaVQqVT47LPPTKp9jho1Cvfddx/at2+PuLg4vPzyy7jrrruwb98+qFSqKvsrLS01SbluimqgTUltr0KvNh7o1UY+N4W280JafikOxmdh1b/yyMAPx66isLQcH0/uB7V91e/SwKMNMONn0dscFAp0vgOwV4uRm9xC4ORm+Xzg5An49xBpY1cOiNEeexdxEZoVJ+/T0yiNTTonpp4W9+5BgIOLGGHybCcXGZBIGRaDnza6AK5IJ5dS7STBA+RAJ+2M/N45iUDy8aoX9cXZcsU0oO4jOlIhAq8QceFqvF6fFDT53yQySPKviZSutEr/P6ob0ck2GtGpKb1MWy6318VP/H2kQMc4Nc64OENRlhjdksT/I645FNdZV6m2jEeeGjrxvnKgU5IjqttJjIM1QFyT/PpfIOxxeV3FuigrBnYuBLqNlhegv56CdGDfcvFz8oma0/mN/45SB7H0fRuXTN/3CdD7QfP8HZoJK7zKbT6WjO8NXzdHxKTm484P/8Hw93Zh1Z54zFh1EN8fTcLCn85A336YmKsjTYrsfKf4B+bdXh4CbjMAuHmGqGhTkCJKVsf/U3U4GhD/cMuMDh5lxcAXw4DlYbWfSKcpBD7uC6waef2a/03F+MBSmG6dbSQiq3ajap+TJk3Cvffei169emHcuHH49ddfcejQoRorglpDNVBr42inwv8eGYBPJvfD5AFt8cTwjnj/gT5wsFPij7OpmPTlfsRnFEJ/vbmnvl2BUUvEvB2pg08KUP5+Bzi6TvwsrbsjzbsBgLZhIk3czSh1TSoYAIgRIgBVghWFAugzWd7upvvknzveLuYauZoG1hgw2/R3aZ5Owj65uEHfiuDG+EJZIl2gStcCUuW42pJS36W25ifL50bpWsG3q0iX1+tE2ljldeiqDXQuyz8bV2szVpQBQC/a7uxtNFcqy3RUIfWMmBQPyEGj2lOk+eUnV00zy08Ffn5KzDWpC+N2NjTQqTzKVZwDXPpb/r1yCeeja4Hzv4o5XfVx7lfg4Be1r0T37zIxugmI78k4eKxJYbpYMPfd9mLut/H3lXJKjIjaEAY6DdCrjQd+mTsEw7v6wk6pwOXMIiz69SzOp4ia8YlZRTh9LV8cxJ/YD9z5f2KCpGTYi0BgH6DvFNGLdc+H4vHjG4C1Y8REO00R8M97wB+vAon7gS+HA+93AVIqeqhObwVyE0WPTmItV2pO2CcObkkH67e6s6UZH1j02uoDPpKVlwLfPwocWdvULSEyu8aq9tmhQwf4+PggNrb6eZLWUg3U2jjaqTCmT2ssua8XXhjVDRNC22DdwwPg6WyP41dycNv70Rjyzi7sjatDJajbXhYLjoYMlR+TgpkeYwEnb/FzyBBx7x4kb2cc6LhVClaM08L7TJJ/Hv4S0H6YuJAf+px4zDjQaTsI6DTCdF9SsYKMGHHxae8M9BgnHks+IS6YS3Lli0zpgtow4qSvfedk3jUg6RAAhUhfgkLMiZFGI4oqzpHOreQO1JRT8oiOtL5RdalrxnN0agp0pLmyzq1EWqBxUQjjUSO9Th55yq2Yn+MVIqfxSSNrkt3vi0B2+0s1fPAaGI/oVC6xXFdFlQOdbNP5RJVHdKQgNuPi9fe750NgWW/T7xcQo33V7bc6WfHAwZXy7+UVlX4V1xklBcRnOP29uD/zgzxyKQX6F624Cp4FMNBpoAAPNdbMGoBjC+7Am2Nvgo+rI7xdHNCvrScA4NdTFf+YfbsCg+aK3hBJrwnA4/+I0R1ADGPe+jzg1V4MyaefB764FfjrLZHStmqk+E9SmgvsWixGdw5+Ke+vctCi14uDXeURkQSjCbmnttT+w579qWpZTnPKSxY36aAq/Wdm5bXri9sl/o5/vs6qfdTiNFa1z6SkJGRmZiIwMLDa562pGqi1G9ihFX58YjAGdvCGnVKBqznFeHjNIXx3JAl74zLEmjwA0vNLUaSppmpWpxHAlE3AzF+BJw8Bo96WR1Ts1cDIxUDbcKDPFPGYcTECL6NARxpNkLQyCnS82wPjVgD3fiLOzxPXA3MPy8GTk5d8QX/HoqptdPUTAZAkoJfYj18PEfgc+ELMP/rkFiAj1miOS5C86GbCXuCnucB7nYFf/lvzF3p+m7gPHgB4BgMuFWsUSQUJpM5AZ295jk3SIXlB0w7DxL3xuTT3qsjuqM2IjvQ6l4qRNkOgkyGXkParCGakJTOkAMSjjVxgwjgo0unE6AYAXN5dt8JDlefoNIRxSh4gRjuutyinFOjkXa15IVSdTqSb5SQAJzaZPpdyUtwXpFYd0Us5LTqvpakFv78gFsttP8w0mHcPEteIgDxCWFlixXpO147LQZU04mi80K6xA1+Kf4vVjUhaQuqZRrlmYaBjJm5qe0wLD8H++bdj70u347GhYmG1bSeTodfrUVhaju+OJCGvpOz6O7r9VeCZ48CEimpCmRW9BtI6AYF9ACiAmG3A/s/k3gFABDrXjomehLISEQytGCJyMo1d/lf++cwPgNaoTalngZ0LRJlFYxf/BL6dDmy2ULW8klxR8nNpNwB6UTlHCgBZkOD6pAo/xVkNP+hbWu5V039vRLVg7mqfBQUFeP7557F//35cvnwZUVFRGDt2LDp16oSRI0c2yWdsaUJ8XLDpsXCcfP1ODO/qi5IyHeZtOYEpKw/g/s/3YvmuWIQt/hOTvtwPrU4PvV5ffYqbbxdg4BzTTsK+k4GHt8sBjoOLnPZmPEdHoTAd1fGpNM+m72Sxtg8AqN3ltX2k187cBjwaBQT3r/5DTv8JmPaDyM4Y/b54jbSsRPRicf7WlgLH/iePprj4iCAKADZPrXguDTiyWg4aKjv3i7jvdo+4lz63NE9HmqPj5AW0qWhrXJQ8uiKlu0kBS+pZsXD5+vtNz6+5V6q/8JReJy0CK/0tkk8CmnxxvpZGyBIq0qKklDL3IHmRUeNUumvH5EBNrxMdqbVlzjk60t9FGu2Qgi9nH3FvPPJSnG06r6umKrmpp+XvLO4v+XGp8xkQ2SqV5wd9Ow34bhaw8zXg0FfAxT9EKuLdH8iL9QLi7+DeWm63exv5ObuKhXylcu1pZwDoxeM9K/4dXDsqpxgaO7lJ/HuQ5gRZ0tWj4vp00xSRlWJBDHTMzE6lhNpeheFd/eDsoEJSdjF+P52C2esOY96WE/jvpuPXz1eWdB0FDHgcgEKkuz2+G3jmJDB7l1iUDQB2vCzugyp6cK4eATZPkxeQ2vepePzIGvEfLD9F9EBItfgdXMUB8lK0/L6/zQP+/UgMKRs7WpEWlRlbdeVoc7hyyHRVYtcAOb/a2ktMJ58QvSCHVzfN+2dckH+2hlKYNbkUDXzYQ0zElMT8Lr47qSKhNSnK4grTVmLixIl4//33sWDBAvTt2xfHjx+vUu0zOVnueZWqfd50000YPHgwvv/+e6xfvx6PPvooAEClUuHkyZO499570aVLFzzyyCMIDQ3F7t274ehYzXIBVG/ODnZY8VAoHhrYFt0C3OCmtsPpq3l4b0cMdHrgZFIuNh1KxOSV+zH64z1IzCyq3xsNnw/0ekA+H0qMU9CMR3Rqw697zVXIAFE5ruPtItUusLd4rNcD8oiL5ORmufqZs49p0Ka0B9oNFj/v/0x0Mhqna+ddk3vgu1cEOlIaX16lER0nbznQSTklLqZbdRYV4gBxTNOWV1SbK5PnajiIQkooK6p68Q3I5+DKIzpSie9WncT8Y0CMzpTkGY3oBFU/onOuYo0a6cL89Naq71sT4+Ny7tXq5/H++zHwwxy5PLm2DNg4SSyOKl2DaYrkdLBWFYGEFHx1G13xu9FcKClIkdSUvnZpl/zz1cPy3yfvqunf1ni0qChLnsO09xNxLQYAg58RKZfGxTBcjAIdv+5ySqaDq2mwbsyzrUgj9Gwrildd2W/6vF4vB9rnfqk+EJLkXBF/r0Nf1+/6TFsG/PK0CHDtnapfosWM6lV1jW7MyUGFsX2D8M3BRDyxQS4L+df5NPx2KgV3964+PcLE6HeB2+bLvT/SkPywF8UFoq5MHFzu+RD4tL8IFHIrThInNsr7yboERC8Rk+dcfMU/cvc2ovT1wS/EBXrnO0R1D+nAd3wjcNur4kBemCneTxK/Wzym1QDtap86UkVhJvDTEyKv2bhiCCBOBFLvkbWnrp38VvSC/P2O6B2USps2FuODbfJxsU6TNZL+DZ3cDNz5lqj4d+AL8d2d2CSvLVUX2jJxwux8J+DoZr626vVijlzeNVGOVjqpUJMxZ7VPJyenKouHkuWo7VV4a5zISricUYgZqw8iIbMIoe28cCQhG6/8IM/dmPjlPjw9ojO6BbihX9s6rGs0cE71j0uBjp0a8GiEAhL2atGWqEXATePFxPb8ZDFnAhDnYCejQCdkCDDsBWD1XeK8e2KTOK71mgAMelrMz9VrRfEDqVdfCnSkC+UioxEdny4iNa60Iiuj420VgZUCgF50blaeo+HTSXSE5ieL+TI5V0SAZ+coskOki3MpgJMCHenc7NtNpO216ixGsWJ3ynN0jEd0MmPFZ1PayaNUt79SMQd5r0jdCugpt0uvFxP/9y0XlcPGfSa+L+MAQVsqUs1c/cRokouvGMX783XxvaWcAh76Hkg9BVyoOAf1nSz2I6WoqRxMR0UAcU45+j9xvZQdL+bLpFYOdC6gWnFGgY5eJxb0LM2X50pJ8lMqsnQglyhXOYhrK3sXsRbUkGfF48YBjIuPSFmLB+DfS/wdLu0S/74rB9kSaaQzZKiYBx6/Wyx1Ism7CmgqgpuyIpEuWV0Z6uJsMRIjdUzH/w08uK7696zJvuXi7+LkBYx658bbNxADHQt6/d4eKNKU46fj16BQAMO7+GJXTDoW/nwagZ5qtG/lgqOJ2biQWoBOfq64o4d/1Z04VXOg9+0CPHtaHCzUFbm+wQPEMCcgDuzSsKV3BxHo/F3xj0k6QIQMAfo/Iub4xGwTkfyVA3L5y8J0cVDoMRY49a0IqiQnvhH5v7pyYM5eubemJodXAae+B8Z8ZJo6cGglcGE7kHS46poDuUkiBxuoX49BbpIYnu9yZ91fez06rQgY2/QXJxBATg/ITxYHG+ODR32V5Ioc2w7Dr9/bYdwLA1jHiM7uD4AzP4rv4ZaHRU45IH9PRRlAygmRyy7lEVc+gdTW3o/FBUWfKcD4z02fk76L+qyBkH1ZTgk89Z0oN2usXCM6ASq7sENUgep4e93fs7KsePF/46bxgIqHamoZQnxcsOO/tyKzUAN3tR2GvrsLOUVlcFApEeipRkJmEeZvFceDxeN7YUpY2xvs8QaktXS8OzbecgqDnxXnrzb9RebFwS/lTAiPNqbVx7qMFNu27ifPqQFEh9DJzeJnOycxl0gidbwcXi3O+1KKu7O3+IxBofKoQsfb5QICRRlinq3x+wBiXrDSXpzDpHm7lXv8Abnz0ThQA8T7KRRixGnPhyKIkaquebQRNwc3keaWGScupLPixPypm2eIEasL24ENDwB3vime9+0uKo6d/1V+n/X3i3nM0gKuTl5ipCz3ijjn/P68yAS592MR5ADi3LJ1tmmgsPdTcQ0kpa05+4jy5ZU/k6ufuJb64XFxzaO0F8+5+IkOOinQuXoEiH5HdOB5BssLp3a4TfwdoqqZ4wWI7/v8byL1Mq2i8lynO0QHt2uA/H0D4t+vxMVXfG+uAeI6Tkr78+4AOLpW/15SR7kU6FSep2NcPQ8Q13nVBTqHV4kgx8lLBD0Xdoi5Xg4u1b9vZYWZYqFeQBTocq0hMDMjpq5ZkKOdCh8+2BfvP9AHq2f2x4ppoeji74qMAg3u/3wvbn5rJx5ZexjvbD+P2esOY/vplBvvVOLsLQc5gFwBRu0BPLJTHDR7PSj+40k828oHqA7DRA9Mt7vF73uWGeWmVvTWHPhCLJa2Z5n4vXvFSEHCv6LHQa8Ddv1fxfo/n1afZ5l+AfjtBVEAYeODcs+TTif+swHi4BtfUc4xrGLBtmEvGo3oGAU6Oq0YGag8h6iyb2cAGx8wzY81h7M/is+8ZYYY2tWWmwYXxzfW9ErTtQhqotcD+z4T+dMbH5TTE2tSlGma8pd83LyT+3YtAd7tUPvJiSV54oCfchLYsxT4piJvu1wj8rklsX+Ki3gpbSDtvPh+6ko6wJ/ZalrBqDBTjMisvqt+VfukoAwQJ/7MOFHVrlwjRvDe8jOthgOIkbWNE4H1E8T2DfX9o8DWR8WcuYbQFJmnPURmorZXIcjTCW5qe7w4qhtcHe3w3gO98f2cQZg5KAQD2ovz1KJfzyAu/TopNLUhjX405kLcSqVYq0dlLzp77J3Fxemwl0Rqm8pe3rbLSBEk3PuJmCw+9XuRot5jHMQoDICRb4nztUSamF6YZhq0SOd3KX1NaScXV5B6+k9+C0AvRlkM2SIhpnObggeKMt5SFTlJ5dQ1AGh9MzDgMfFz9zHi/qLxiE5r8fmk9qefE+n0gOhIVbsD4z4HfLqKtLHvHxElp7+OEEGOykGMavR6UAQvf78tXuvRRm7zPx+IIAcQgc8/Fan3IUPFdxD/N3Bis9zmC7+L47WU+ufiI5cvB0Tg4xYo/9tJOiTupQ5faa5LxkVxvv31WeDiDhGYJewVo0xugfJ8rcqkQkvXjol5WhsekAOPgJ4V5c0rBQCVU9e824vRMGdvsS7O8JeBEa+Zjug4GAU90nclZU5cO2p6bpQ6TAMqUjAv7RLXdWnnRdp5Zpy45tm/Qjw/6m1R4bC8pG5V3PYvF+WypYrDjYDdhBamVCowIVQeEv328XAs+uUsth4TB4FOfq5wU9vhWGIOIr89joLSnojo7gdP52p6i6+n71RRMKD/oyJyfyxaPF5eKv7TFmWIA6lHsPhH2+tB8fyQSHEwObkJhoPq2OVigljCv8DXFQvs+XQVKXIXdoj/xJJzv8hD0JkXxaiNRK8HtkXKB4esODHR7qEfxL4rV3ixdxYR/rAXxQFYWj/BeNHQ6CWi3HbXu4HJNQQV2ZdFXiwgVgtufbNoo3RQrQ1tmRgq92onRh1OfyfmTJ2tyCsuyQWOrQdCBouLdYVKHITP/SoOHtWNxH3/sOi9mbRBnNwkJbniotm7g7j43280EfD4RuD216r2NkmkHiXXAPE3LsoUo1nSKMqRtSK169YXxJoTOp2YpKpU3XjUIfFAxUigXvTATNpw4+8t5jfx78MjuGJU7bScEmH87yY2yjSw0ZaKfz83Gh00lntVDsDKS0RqyC0Pi9/P/yIHUfG7q6bz6fUiB7owQ5xkHZxNn086LP+cchL48jZR7TD5REUPox748w1xIZB2VoxOHV4tHtdrxb/R8Stq/1kqy4qX/w3vXy5GpXo/KALJ8pLaLw58+V/g57ni3+d/9oi0GiIrMnlAW0weIF9kv37vTdDp9Ji26gD+jc3EtK8OYOrAdjiWmAONVof7bw5CW29nlGn18HK2R3sfF9iprtNn23ui6K2uKbXN0vy6A/MuiNQ5KcAxTr2S0tECeonULMmDa8XIT941OViR9BgrLoydveW5uIB83uk0AvjnXXGMl1J6XXyAdIieegDoMkpcCO96S2xv3Ek35iPAr5voWPzwgNxe6bjj3UEe3Zj4P/m40vpmEYRJ83MUSjlY8OsmjmlJh8UoOQCEzhT3zt7AtK3A97PFucDBVXQgegSJ65Ggm8UxO/eKnF7vGSw+bxJEVgpQMWqVKVd+6zNZtOfkJnFx7eAmzoOxf4qRCWXFJXDrfqbn2MDeIjhzby0XfFIoReeuQimC1QMrRCrepWj5PHRxpwjMAJHR0P5WoN0QwKWVuJeCsbbhovP3wh9in9pSubJe5ewWiVeI3IbK6WkOLsDwF8XPxs91GSmnTEqBjkcbEeSmnxcpdlLQJo3odBklOrV3vQX88Yq4SZR28tSHnveL8/veT8T11U3jxDblpTVnoRRni8pugBiZa6RFSxnoNDJPZwcsndgXz97RBY72Svi5qVGu1WHm6kPYE5uBeVtOwNFOiTnDO2JoZx+k5JYiNa8EQV5OuKO7P5TKGv5huLcGHqkm59zOEZj1mxhJkObTGA/htgkF+j0kLtoBkWPbZZTIufzzdfEfuW04MGmjOBi1DRMjOO2HiQPYSaPSiUfWiF6RdoNFUHN8o7hItFOLKnLfzxYHhX2fygURpJxeQBwkVXbyRE0ptzr5hDjYl5eKCYaAOLAlHa46UVSvlw8YgOhdiXpDHNSObwSm/3jjiW/pF8SITdpZkTIgXTAnHjANzvYvl1OK2g0S33HaGeD4N6ICzenvRV6uXw9xEJVGH7Y+JsqKewSL9ICfnqy6kNrIxeJvknZWtLvbaOCv/xMH+dHvi2IVgBzoBPQUi6+lnhLvE/6kWEx2x8si7zbuL1EKNT9ZXkV8+k81r8xcViJ61aSF9s5vExffUiU8YzptRUCkkE8w/aaJgCDlpDghS6NOPl1FStiVg1VLc6aclgMdvV58ttIC8e8h75oIUqXqg4BIdQBgyD0/tl4OdM78KG93aVfVQOfKAVHVBhAH2/u/Fj1acX+Jk5wUZEh//9KK0aLDX8v70OQDKwaL1Ab3IJFuITm5GRg6r2qVJ0AEtns/EeuEtAk1fS75hBiBkdJG7J3Ffn/5r+gR3TxNnMj/s6f6v4WxA1+I8qSAyA3Pjq9bIEnURJRKBT54oC/u++xfXMstwXs75PTcfy6Yztns5OeKb2YPhK9bDcd1z2Dg/q8s2dwbqzx/cPh80QEzquZ1nQCIgMK42pZhf65yqq6LjzhXA3LA0Xag6Oz0CpFf41JRRUxK6ep8pzhvhT8pOnrKS0SWRdh/RFACiA6xPpNEOprxPtz8xTHIuZXpyINCAdz2iihmpNOKi18puPOtOPYc+locU1t1Fu8v8WgDPGw0F1ivN70QViiAiDeAVRXp6DqtacnltuFA+FwxQiIJGSKO59J1SucIoOcEEehc2C6PTLUNNx3RkUY1jBeibTdYjFzpdeJaxU4tvrPfX5S3KUyTM1VuGif+HrO2yZ8n9bQ4Dne7RwQ6UuEDsYG48zeao2TMzkGMoGTHX7+jyzjQ6THOKNAxKrveKUIENhd3GgU6FdcSvl1FEFNWKP7u0ty23CT5WmjwM+Lv2n2sOJdd2CGuz46sEeecyZuArneZtqsoS6QAaiquibreXfNnMDMGOk0k2FvuQbZTKfH5Qzfjy38uYceZFFxILcCyPy9i2Z+mFT36BnvihVFdMSDEG+eS89HaU41WrqYH99i0fByMz8bE/sFQSUGR8ZB3dcYuFweI2D/Fha9CIYagu94tLrR9u8kX9EOfE/9h73pHjBQ5uIjeoLSzYr2ff5eJm+HDOQF3vy9S5EYtBn55RpROlNz7sUhvKsmtGrSEDAY82ooFUb8eKU4W2lJ59OTX/4qTgE8X8R/39xcqShcbHRyTT8gX9ol7gTV3i3YH9wdunil6Wk5vFf+he08UwdWv/62YlKcQ/7GVdqL3SiqN6d5GPJ6TKA+Rt7lF9Jj8+qwo8HBhu5yOB8gpBUo7cdH/cT9RbUSa/OceJL7XglRRSvKWWSIg2/acSJWTJqQC4kB+91Ixd0MqRODTRVzEpp4SPTCxf4rPoykQuczaUnn9JKlX6JdngPtWijlQPl1EAKxUiRPID4+JgMTFVwyZJ+4T23sGV0yqDBRrS9g5AjteAQ5Umh/T8z4xGTblpAiMdRWjNz3GihGm9PPy4nEdR4hRpmvHxEX91SNiFC0jxnSfUIiAWTowS8UNBj4hvvOrR8RQe5/Jpgu+SemL2nJRwtWnUoW8Mz9UBEYVJxo7tTxX7dbnxL9rj7YimIz5TTzeb5ooCyvNhTNUGAoWf4eLfwDrx4vRosq9sX++LgLvvZ8AD6wVQateL1L9ot4U7TBev+NMxQjoV3fIo2J/v1t1TlJJrghQHd1EQPvHa3JbR/6fXH6XqBkI8FDjz+eGYeOBRPx9IR192njCTqXAT8evQVOug71KgdS8UsSmFeDhNYdwRw9/qCoyKPzd1dDr9VA0Uo9xnQUPAF5NM8+coUHPiFEE49QzQK60JpGqmwHiQrvdIHGul0azO0UAz12oehHdd6oc6EiVUAE5GKqs31Rxq0xKp5Mulm95+Po9+tU91zZMdHalnBLXFGXF8nNjPhLfgdQ55BEsz0vpPFKklvV6QIyyKO1F56K0mGe7cNN9SQUCjNdn6jDMtMPMt5sY7cmIEefUgN7id61GjDS1H1b189wrddRuR7XsXcR8qZqM/D8xCiNV6auO8d+vTX/RjuzLckEIQAS5+z4V1wk6nWibNKLj260iqHwdCJ0l5rjZOYrtMivWg5IC1KBQ0YmWf00EVLs/EI+fqBToFKQBX0WI0twqB7kYUSNR6GtV67hp5eXlwcPDA7m5uS1+kTa9Xo9tp5Lx0Z8XUVymRYC7Gj6ujth9MR2FGnGh62inRGm5Ds4OKrx0Vzc8FNYOSqUC2YUajFz2D9LyS/Hq3d3x6NBqeoIsRacTvd2xUeI/lXd7USXm5unyCI1eL4KaC9vFEHLEQrEIXNSbIjh6ZKcYojaWkwisuUeuXa9QAZO/ATZNNS2QUB0nb3l9AUd3MWFOChYA8R8uOEyUw6wsZKgIAoqzxWjMiU1iZAgABj4pRtCMh3QnrhcX7Eu7y6MXdk6iGII0kqNyBGb8DPzwH7nKnIObGK0Z9ba4EC3Jlb+v0nxgaQ+5ek6H4eIiVkoVVNqJi3JNgUgr7DNZpEzt+0ycTOxdRK/Mrc+L0YPseBHIBA8UBx1psqjETi2CAG25COqU9sBD34kgZf39Vb+jQU+L70aaaClViwnoDfxntziYfzNRTnPIiAEmbxYnj1/+K0b1/HuKyZS/PisHsIa/j6OciqD2EL1lKgdg6nciMPuoj7jwf2K/+PtIAbZHsEhxaNVZfGZdOfDUUREcSD17UrA36CkRcEjv5+QpBy9qT+CFePHvNShUfHerRon2T/tRBHjJx0WP6K+Ron23vSoCsXXjRIAOiO8+YqEIgHKTgI/6yv92FSoRzFw5IJdblSiU4sKjJFeMHJWXyN+RQgk8cUD8va4cEPPopIpCdk7i5JQdLybDTvuhQSkCtnT8rQt+L03vUnoB7v98L7KL5HOBvUoBRzsVdHo9Zg4KwZO3dYKLox2yCzXILS5DiE8tJ023NP9+LHcyvnSl9mncgDhGlhWL6nANcSlaXCwr7cX8DOO5SrVVWiCOyd3uFqnHmyaL+US3zBLPb35InCN7TwLu+0J+Tfp5uTN17Ri5M8ytNRB5VqRYL60IBp46Kjr+jm0QlWEBsZ6ScWfs1aOiwyrrkqha6+QlOgMB0bk01iilsLJrx4EvjQIhKSWsTX/g0T/r/p0YSz4JfDFUpP/NrzjH67SmBW3KNcC77cW1Q68HRAfx2R/FeeXl5LqlOP/9ruiMlQJMQHQmPx8rzjs6HbDhftHh6NlOpDpKgWQD1fYYzECnmUjLK8Enf8Viy5ErKCnTGYIdABgQ4o15I7ti9b/x+L2ioIGnsz3+eeE2uKvrcSCxJE2hmMfSYbi8kJteL/6T13TQK8wEzv0kDlat+4oemYMrRe960C1iInpmrKhWkhUnDjxBoeJiWMpHDvsP0HW06PW3U4vUIuPFVrvdIw7CmgIRGAyfb1omWlMEfBIqei4ejRL7/+15UTkOACLPieBn5wKxDhEAjFwChD8h8pF3LhQ54oPmioNOQaq4gG3V6foH+4R9IiDoMkoceKU0sVNbTNPdHvlTXtTu1HdiQqdk7pGqKVSxUWLivIOzSG/IiBVBkUShBCasFsPver1YfDYnQbS3vLTqOkt3LBLfSdSbYuJo11Hi870TIo+OSBfurr5in2nnRO9TVryYeCoZ+KQIeDvfIY9C6LTAlpkiGHBwA/x7iAv8oFDx9wBEqtaOl+VgacRCMbKSuE8OfoyFDBUrr+ddE21zbiXa9OUw0eaOI0TeeG1kxIpUvbD/iJNESZ64qDi6rmKtAGcxR+7iThFstRsseh+lf5+AOPnf9Y4YzTr1rcivn/ZDxWf7UoxYjloiUl5itolA1tFVDszElwzDyJTKEXhiX83rKtQSj7/V4/diHU5cycH7f8TA180RiZlFOJxgWnzE09ke4R1aYVdMGjTlOqya2R/Du/qhtFwLRztVDXttgYpzRHrvTePF3JeWKu2cOA9FLKw5m8U46Ot5v8gU0GmBzweLFLHZ0WLE4VI0sG6s6Ch9If761S9zr4p14gBRzvp61VfzU4EPusi/958triXC54pRm4bQ6cS5IqAXEDqj5u2+mSLPbZJ4dwSePlr99jUpzgGW9ZI7ZCVP7Bep6n+9KbIV7JyAx3aZNX2agU4LlV2oQXJuCTr7u+Kbg4l4+/fzKNLIveAqpQIB7mpczSnG47d2wPzR3aHT6aHV62GvUmL9/gR8fzQJb47tiZ5BLSSVpVwjAhzfbiJlR0pDSztbMc8EYrRIqkwnST0jigt4hYi6+gXpYgTHt0uVtwAgRqpyrshVS3Q6YM8H4oJSKj+cdw34crjosZi82bLDs9kJYv6J0t40VcB45CwoFJhdQ+W50gJxAa5Uis+SHV9RRUYrvsvrXSD/8oxcOeeORSJntzpfDperAt3xZtUyzYAIfhcHAdCLgg+j361+X2UlYmRJSsFTOYi5TsYHztwkESgUZYmD/L8fi0mVgAhm7lspht4PfQWM/UykQlS2bZ446YxcLEZrGiIjFvjtOdNFeQFgxi8i0Nq3XJxwXfxET1fwAHltoLaDTFMnyopFumP6BeB/4+R0OQdXkYs95FnxN4v/R+SJdxkpLy7cADz+Vo/fi3WKzyiETq9HbFoB/m/bOSRmFZk87+vmiH7BnvjjbCp83RwxtJMPnhrRGf/GZiA2rQBPDO8IlVKBo4k5uK2r7/ULHVDzlHYe+Kzi2D/6fZFZAohsBoVSPm9ry0Uqe7vB4hrhRnYuFCNDY5dfPyjSaYE3feTCApHnRUdZx9vrNtLWEPG7gR/niGsE7/aicM3N00QWTl399ZbIJkFFdb3086Kj8coBeS7tmI+vH3jVAwMdG3ElqwhvbTuLE1fEZOnHh3VAGy9nzF4nJlNHdPfH6au5KNPqMPvWDnhn+3no9UCAuxo/zx0MP/cWXIUp54pYSNUrRPRsN1a+tvRfqinzwwvSxYGn7+SqudrmoCkU85OCbpZLilZn/wqR4jdiYfVBjuTvd8XIzj0fXn/YvCQXWD1azO+5XoAlKc0Hdi8Vo20db6/d6IZOK0b+pOIYDaUtF9/BsfViZKnvFJEyJ/37yLsmRq5quw6BtM/MiyKtL+gWi1ZT4/G3evxerJ9Wp8ee2AwcuJSJm9t64Z3t53Ex7frlqt3VdijX6VGk0WJGeDu8MbaGyeHUfOn1wGfh4hj6xIHqi8ZY2gcVHbMdhovCQM1ZcbYolNOmv8gS+estGLILVI6i81KqsGdGDHRsmF6vx4c7L+CTXbHVLqlir1KgTKtHSCtnPHlbJ4zrFwT7ltprlZ0g5rVI816o8ZWVmPdCvKxY9BhZIoCzpMpVhJoJHn+rx++l+TlzLRcPfXUAbbyc8da4nigp02Lpzgs4EJ8Ff3dHeDk74HyKXA1SqQC+ntkfxxJzEJOSh/T8UqjtVbi9mx8eGdIepeU6JOeWwE1tB59KhYHIyuUli46igCYKZKVsB3Okq1mTxANyZTx7Z5G9ULnQlJkw0CHsjcvAhgOJGNbFF3/HpGPbqWR09HXBp1NuxrSvDyKjQFRw6hXkgTfH9YS3swMCPNRwsFOioLQcdkoF1PY2lMNMRFXw+Fs9fi/NU5lWZ9Kxp9frcSG1AO1aOUOlVOD7I0nwd1fju6NJ2HYyucb9RHT3w7HEHGQWagAAQzr5YOnEPvBza8FZEmQ+P8wBTmwEJm4Aut/T1K0xn3KNKMpUnCXKTBuvGWhmDHTIhE6nx/74TNwU6AEPZ3vkFpdh08FEfBYdh9xiuWKNm6MdOvi54vTVXDjZqzBjUDv4uDrCw8ke9/ZpbchX1ur00Ov1zF8mauF4/K0ev5eWLTm3GBEf/I1CjRah7bxwT+9ABLirEZdegA92XjBkSzjYKaGpKAzk4WSPHoHuCOvgjdlDO8DFkSt4UA1KKhagDhnaLEf6ryszTmReWHi0jIEO1UpKbgle/fEU9sZlQqvTGyq5VWdIJx/cd3MQ9sVlYseZFDg72OHDiX0R3rFVI7aYiBoTj7/V4/fS8sWm5SOrsAz9Q7xM1uT57VQyPouOxZjerTFrcHskZhXhyQ1HEZMqp735uztiengIBnfyQbFGi1NXc3AtpwT/GdYRAR4c9SFqKAY6VGc6nR4nknJwKb0Q/UO8ceZaLr47kgR7lRL/XEw3qe4mUSqA/iHe6NHaHSO6+aNMq8O13GLc2SOg5pWqiajZ4PG3evxeyJimXIdjidm4lFGIz6PjqlR7kwR6qHFvn9Y4dDkLgzv54NYuvth08Ar00KNnaw9M7B/MkSCiWmCgQ2Z1PiUPb/56FqVlOvRu44kR3f3w47Gr2HIkqdrtvZztMaK7P45fyUEbLydEdPfHA7e0gaOdCvklZfjrfBpKy3QY1SvA+tb6ISIDHn+rx++FalJSpsVPx6/ip+PXEJdeAGcHO3T0dcWljAJcSi+87mtvau2O1TP7t+yKqERmwECHGsX5lDycvpqHQ/FZ2BWTBicHFVRKRbUH846+Lujk54pdMemGnGYnexXu6hmAu3sH4rauflAq5fSAvJIy/HEmFTe1dkf3QP7diZoCj7/V4/dCdZVbVIaXtp5EkUaLIZ18sGbvZSTnFmNc3yCE+Lhg3b7LyCjQwM/NEf+N6II+wR5wdrBDW29n7L+UiT2xGZge3g6BHk4m+9Xr9SapdUS2gIEONRlNuQ7r9ycgKbsYA9p7Iz6jEF/viTdUeQOADr4usFMqcCFVXtOgW4Ab7u3bGmXlepxLzsPfF9JRXKaFnVKBhwa2Q3aRBj6ujpg5KARf74kXk0If6GNzPV/Hr+TATqloOQu+klXj8bd6/F6oocq0OuSXlMPbxQEAkJBZiFlrDlXpKDQueNAtwA1P3NYJ7++IQblWBycHFa7llKBfW098NKmfScr4yaQcbDuZjIeHtIe/jZ0nqeVjoENWJbeoDKv3xkOr0+OunoHoHugGADiSkI1fTybj+6NJyC8pr/I6PzdHpOWXVnlcEtbeGxseDbOZ6m85RRoMXBIFe6USh16NYPlvsjgef6vH74UsobRciw37E7F+fwIKSsuRV1KGkjId1PZKONqpTKqkVhbs7YSvZ/RHF383pOeXYuSyf5BVqEGQpxMeHdoeF1ILMGVAW/Rqw04yav4Y6FCzklOkwdq9CUjMKoKdUoGOfi4IbeeNm9t64ucT1/DH2VR09HHBH2dTcT4lH0GeTsgp0qBQo0WPQHd4Otuji78bVEoF4tIL0M7bGb3beEJtr0LvNh4I9nZGuVaH3OIyODvYwcmheQYIey5m4KGvDwAAfnpyMPoEezZtg6jF4/G3evxeqDGUa3W4nFkEXzdHxKYVYPKX+6HR6jBtYDvcd3MQijRaqO2ViPz2BBIyi+Bop8R/hnXE4YQs/BubWWV/ansl3pvQB7d29oWHM+fHUvPFQIdapHKtDgcvZ6F3G0/sOp+Gp745dsPXqJQK3NbVD0cSspBdJHrDIrr7YfH4Xs0u7e2Lv+Ow5PfzAID/G98TU8PaNXGLqKXj8bd6/F6oKZy+movsIg2GdvY1eTyzoBSR357A3xfSDY85qJRY83B/rPznEgpLtdBDj0OXsw3PB3qo0T/EG48P6wAXBztsOXIFpWU6pOWX4vDlLHT2d8Ord3dHZ3+3Rvt8RLXFQIdswuHLWUjOLUFJmRbnU/Kh1enR0c8Vsan5iE0vQH5JOU4m5Vb7Wgc7JbydHdDW2xl923oiq1CDwtJy+Lg6YlDHVrijhz/yS8qhVCiu2/OVkluC93bEwN/dEUM6+SC8YyuLTQx96ptj+OXENQDAlLC2WDy+l0Xeh0jC42/1+L2QtdHp9Pj28BXsvpiB/NJyTAhtg3v7tDY8X6bV4e3fz+On49dM5swqFYBSoUC5rurloEIBBHk6IbSdFyLv6AJ3tT3i0guQV1IGtb0KrT2cEOztDJVSAZ1Ob1JQiMiSGOgQVTgYn4XomDTcEuKFoZ19EZdegOe3nMSpq9UHQBJHO6VhAdVWLg7wdXOEm9oOCoUCI7r54dGhHaAAMHnlfhyIzzK8blgXX7w5tifatnK+YdtKy7Uo0ehqnUJw+/vRuJQhJqr2CfbET08OrtXrzOHMtVy88N1JzL+rO4Z09mm097WkIk05VkTHYXTvQHQL4LGlOjz+Vo/fCzVnhaWiE3DDgQT8ejIZgDh3dQt0g5O9Cn3aeGLjwUTsPJtqeI1KqYC2mmDIyV4FN7UdMgs1uK2rL568rRN+P52CmJR8FGnK8dydXTGwQ9WFxXU6PS6k5aOTr6vNzLMl82GgQ3QdOp0eCVlFyC8pw9lreThzLQ9+bo5wd7JHYlYRfjx2FZmFmuvuY0CIN9q1csaWI0lwdlBh1E0B+PVkMjRaHRztlHjq9k4Y1y8I3i4OyMjXINBTDXujg3lcegEe+uoAcovLsO7hAbglxPu671dQWo6eC3cYfne0U+LMGyMb7QTx/JYT2HIkCUM7++B/j4Q1ynta2jcHEzF/6ync3s0Pq2b2b+rmWCUef6vH74VaipNJOdDrUe2cz/T8UlxMy8fn0XHYfTEDgBjh8XKxR1GpFldzig0dgjVp5eKAH58cjPMp+fB1c0SfNh5IzCrCS9+fwr5LmRjQ3htfzbgFbo52eP+PGGw+dAWzh3bAo0M7QFOua7ZzasmyGOgQNUBJmRaJWUUI8hTrFcRnFBpS267llmDpHzEo1GgN20vzZS6lF+DVH09jb1zVSaC+bo4Y3TMAns4OKC7TYuvRq4b0AQ8ne3w6pR8GtPeGo131B/VDl7PwwIp98Hd3REFJOQo1Wvzx7K3o0kj500Pe+QtJ2cVQ2ytxYuGdNbazOXnjlzNY/e9ltGvljL+fv81i77PzbCo+irqADx7oi64BzSvfncff6vF7IVui1+uRmFUEd7U9vCrKYQOAVqdHfEZBxRwg4JUfTuHMtTwM7eyDu3sFYu2+BJxLzoNSAUiDQW5quypVVkNaOSPIy8mkgIJUVjusvTdeubs7erfxrNIurU6P138+g38upuPjSf1YoMeG1PYYbNeIbSJqNtT2KpMAovKaNRHd/bDlcBJi0wrQwdcFUwa0BQB08HXFhkfD8OPxq1jz72WcuZaHcp0edkoF0vNLsXZfgsl+uge6w9FOieNXcjDt64NQKEQagI+rI3zdHHE5oxAKhQL33RyE4orAqleQB3KLy3DocjZOX81Fu1bOSMouRqCHGs4OtfsvfSE1H/O2nMBtXf3w34jON5xTdCWrCEnZxQCAkjIdjibkILxj1VSE5iauYr2KpOxilGl1JiNu5rTxQAJOX83DLyeuoWtAV4u8BxGRpSgUCrRr5VLlcZVSgU5+8rny57lDkFlYCj83Ueinf3tvjPlkD4o0Wvi6iU66/JJyKBRAeIdWmDEoBK/8cBqXM4twObMISoWYf7r16FUUVZzzDsRn4d5P/8X4fkHwc3PEkYRs5JWUwdPJAU4OKkMBhsf+dxhbHh8EDyd7ONor4WinNJzbdDo9YlLz4e3iwDWFbAxHdIgsqKRMC41WByd7FaLOpeLw5WwUl2nh7KBCkKcT7gttg3KtHm/9ehb/XExHRsH10+UA4JkRnZFbXIY1ey8DgKGnzF1th8kD2hoWjFMoFAht54VOfq44mpCNYG9ntPdxgVanx32f78WJKzkAgLt7B6J7gBv6tfXC4E7Vz73ZcvgKnv/upOH3p27vhOfutMwF++HLWdhxJgWRd3S1eMrC4Lf/wtUcEcDtmjcc7X2qnsjNQRoNG3VTAFZMC7XIe1gKj7/V4/dCVDtnruUiPqMQd/TwR5lWj0vpBejg6wpXR9Exl1WoQXRMGhIyizC0sw9uCfFGbnEZ0vNLYKdU4uOoi9h67GqN+1cpFQhwVxuO5ZIgTyc8dXsnXMooxA/HriI9vxQuDip8MqUfbu/mX2U/u2LSoNfrcVtXvyqdf3q9HtEx6VAogOFd/czwrVBDMXWNqJnR6/XIKBDpcal5JUjNL0U7b2ek55di67EkXEwtgFanx5fTb0FWoQazVh80pM85qJTQaKvPk1YoAL1eBEQT+wfD0U6FNXsvw8lehdJyLYznlj4Q2gYP9g9GSCsXeDnbG+b/RH57HFuPXkWQpxOu5hTj5rae2PqE+Qsh6HR6DHt/F65kFeOV0d0x+9YOZn8PSbFGi+4Ltht+Xz2zP27rZv4TWJGmHD0WiLlVnf1csTNymNnfw5J4/K0evxeixnMqKRdf7r4EO6UCt3bxgZ+bGpcyCrH/UibG9w1CZ39XzFh1EJczi2rch9QpqFAAs4d2wF09A7DvUiaCPJ1wOaMIH/55AQAwpJMP3hzXE+19XJBbXIaLqflYuy/BUPF082MDEVZNcQVqXAx0iFo4aQFUrV6PVi6O2HEmBVHn0qDV6aBQKFBQWo5/YzNQpNHC390RqXmlJq9/496b0MHXBT8cu4qSMi1+P50C46OBQgF08XND32BP7DyXiqxCDd65vxde/P4UVEoF/m9cT8RnFOJsch76BnsirH0rdPB1QWpeCRQKBfq08ahzme3dF9Mx7euDAESK3i9PDWnw91STM9dycffHewy/LxzTA7MGtzf7+5xKysWYT8X72KsUOLdoVLOqMMTjb/X4vRBZF61OD025DiqlAsUaLTYeTMQX/8ShW4AbHh3SAeEdW+GtbWfxzcErNe7DXqVAmVYPB5USt4R44WB8VpWy2x18XfD7M0PhaKdCcm4xomPScTG1AP3aeuK2bn5wdbRDWl4JziTnYVhnX5bcthDO0SFq4exUSrRydTT8PrpXIEb3CjTZplijRU6xBgHuahyIz8KWw0nIKChFex8XPDSwHVRKhWHhuYPxWfji7zicS87DtdwS6PVATGo+YlLzAYgqb2P6tMbXe+JxIbUAL209ZXif3Rcz8AliTd67T7AnHgpri35tPRHo4YTiMi0upRciNa8EOr0eN7f1QhsvJxRqtPjrfBpUCoWhxwwATl0V6Q61SSf782wqTl7NxUMD2xpyw29Emp8juZxRWMOWDXMxLd/wc5lWTOjt4OtqkfciIrJVKqXCkO7sYKfEnOEd8Z9hHUw63Jbc1xt39gjA4t/OITGrCEM6+eBiWgGu5hTj5dHdMaKbHxb8fAb/XEg3FBVq7aFG1wA3zBzcHs99ewKX0gvx+P+OIKSVC9bvT5ADoX8BT2d7PHV7Z3weHYuMAg0m9Q/Gkvt6Vdvpp9frLbbmHsk4okNEVZRrdcgs1OBoQjbOp+SjtFyH8I6tMKyLL1JyS/Dt4SvYFZOGAHc1BrT3xpEEURghIasIvq6OyCspQ0nZ9UuOAuLEBKDK2gztfVwQn1GI/0Z0xjMjRLGElNwS7I3LQBd/N5PiEH+cScF/1h+BTg84O6jw6t09MCWs7Q3f+8OdF/BR1EXDekm3dvHFuocH1PGburF3tp/H59Fxht+/nBaKO28KMPv7WAqPv9Xj90LUvEmBhl6vR6FGa5gzpNfrEXUuDbHpBRjRzQ+djQoTbT+dgic2HDFJ+e4b7ImbWrtj98UMJGZVTZ0L9nZCuVaPCaFtcEcPf3x3JAl/X0hHal4Jvpx2Czr7u2L1v5dxU2t3RHT3h7ODCp/+FYtfTl7D4vG9brj0hK2yaOra8uXL8d577yElJQV9+vTBJ598ggEDqr9AWLlyJdatW4fTp08DAEJDQ7F48eIat68OTyhEzYNWp4dKqjC39zL2XcrE2Wt5KC7TQqEA2ng5IdDDCZpyHU5fzTX0hEm50FmFGvQN9sRDA9th3pYTAEQw5GyvQn6pXI70tq6+GNs3CMm5JVj25wWUluvg4+qIjIJSKBTA2lkDcGsXX8Sm5WPPxQxE9PBHGy9nHLqcBR9XR7T3ccHcjUfx68lkjOjmh6jzaRYrMf3o2sP481yqIT/8hVFd8cTwTobn80rK4GintNpy3Tz+Vo/fC5FtiknJx1e7L+FSRiHm3tbJMLdTU67DB3/E4It/LmFQx1YY1TMAC346c919eTnbw93JHgkVc4uc7FXo4u+KE0liQXN/d0f89vRQtHJ1hF6vx4mkXOw8m4LebTwx8qYAnEvOQ0JmEW5q7Y42Xk4mI0TFGi2e3XwcOcUarJk1AGp703OMXq/Hwp/PIDatACumhcJdXbuFy62FxQKdzZs3Y/r06VixYgXCwsKwbNkybNmyBTH/397dRzV933sAfydAQpCHiAHCo4Ki1AdEoTCqbu5IBY/2qq231uOqc129Vew5Vmw3z65i1weq7Xa7rg7venaH225t61brnXV4FARnRVSKraJSabEoEFAwJDwmJN/7B/LTDCwPURLC+3VOzpHfU76/j/nlk0/y+36/5eUIDOzZkXflypWYNWsWHnnkEXh6emLHjh3Yv38/ysrKEBoael9PhoicU0tHJ9zkMps32nazBfpWM6xCINjPE8aOThy+oMOsCRr4qjyw6J1/9uhYGqP1wVd1Rvzr5Nw/nBSA369KwNZPLuCDM9fgo3SHv7dCSh7+oxSYNUEj3RqX8lAgymoMqG1qx2tLp+IX+y/ATS7D5VfSpCGmG1tMkAE2c0YMxtw3j+FqQyuSo8ag6JsGPDEzDL96cjoAoKK+GUt3fYZJWh989B/JTnkvN99/e8e4EFFv9K0m+Kk8IJPJ8MU1PW61mqBvNeO1Q5fQ2GJC2hQtFseF4J38K7hQbQAABPt5QuEul3KWXNY1916doQNRmlEI8/fCxZomm5FZ504KQOFXN6S+tTMi1FiRGIF2swXucjk+PV8jzUv05rJYPDEzDKXXbuFkRQOiArxhFQLP7y0FAPx0diT+c9Fk6dhmixVnKhsRP270sP8SbsCFTlJSEh5++GG8++67AACr1Yrw8HA8//zz+PnPf97n/haLBaNHj8a7776LVatW9es5mVCIRh6rVaDZ1Ik2kwWtt28rCPBR4psbzdhXch0F5TcwSuGGJxPCsXRmKDzc5Gg3W/Dkfxfhy9vfhrnLZdB4K6EztAO4MwLd3Y5tnosFvzmOdrMVDwX7QuEmwyilO4orGwEAz8yOhJ/KA1UNrYgO8oafygMdnVZE+HthlNINV+qacaW+GcZ2MzamTESIWoU6QzuM7WYIAaS+fRxWAfxy8RRsO1CG6eFqHEjvGrHup3vO4OilegDA7h/FI22q893Sxvff3jEuRDQQ7eauXOZ/+8uzqoZWPLH7JJTucux99nsIG63Cl9ebcPRSHZIix0Djo8CSXZ/Z3Aau8nBDbJiflJ8AYEKgN67ebOkxaMLdYsP8ED7aC5+er5WWuctl0j7uchlyN34fEwK9YbEKrP/fEhwuq8OcaA32rEm0+RLu1DcNqDO043tRY2zmJLrZ3IGSb2/hBxMDevx69CA8kELHZDLBy8sLf/3rX7FkyRJp+erVq6HX63HgwIE+j2E0GhEYGIh9+/Zh0aJFvW7T0dGBjo47I0QZDAaEh4czoRBRn/StJhwrr0eQrycmB/tC4S7Hz/52HqVVt/Da0mkIVatw4Fw1zlc3IUrjja2LHsKC3/wTl3XGvg/ehwh/L8SFq/F/dw2qAAB+Kg/sey4Z8//rOBTucsydGIAQtUqaCwnomjz2rX+PhaeHG8YHeKPV1IlLtQZ0mK2YqPWB5q6BJ4YSP9D3jnEhInu1mSxwd5Pdc7Lqan0bzl5tRLvZggmB3pgS4gdPDzd8eKYKH5y5hp/OjsLC2GDUG9vxh39WorRKj9GjPNBpEbAKgRWJEdjwfqk0/YTCTY450RoUfnUDnVaBCYHeCFWrUPhV1xeHM8eORkenFafvKqS2LIjB/Cla6Jrasfd0lU1++/7EACyJC8HFGgPeP12FVpMFc6I1eG9VwgMvdh5IoVNTU4PQ0FCcPHkSycnJ0vKXXnoJhYWFKC4u7vMY69evx+HDh1FWVgZPz95HR9q+fTtefvnlHsuZUIjoQci7VIePS6vxyPgxGDNKgXpjB2ZN0OCbGy34/fGv4adS4KFgH1ypa0ab2QIPNxm+udmCNlNX8okO9MHRS3VSR1SZrKu4MXda0WKy4OnvjcXWRZOR+PpR6FvNNs/9b9NDkH+5Hs139UGaGaHGlbpmqV+SysMN6T8cjwmB3risM+J0ZSPGeCsRF65GhL8XgK4CLzrIB0p3Oc5+ewvjA0YhOWoMZDIZOi3WQQ9p7Swf6AfSN/Tjjz/G66+/joqKCpjNZkRHRyMjIwNPP/20tI0QApmZmXjvvfeg1+sxa9YsZGdnIzo6ul/tcZa4EBF9lxc+PIf9tydczXp8GlYkRuDcNT32nb2GNbMioXSX40d/KJZumwO6ctii2BCbkVC7ucllmBjkg8s6Q487JLrNidbgl4un4n9OVKLomwbEhvqhuaMTX15vgsliRYC3Eotig7FkRijCb+ewgXLKQueNN97Azp07UVBQgNjY2Htux190iGi4qdG34Sc5Z+Aml+HVJVMxI2I0gDsDNABdQ1h/cV2PhmYTTlc2oqPTgl89GYcPzlRhZ245fJTuaDVbpFHoAnyUULjJe8z43V/h/ip0mK1o7uhE2cupgxrK1Bk+0A+0b2hBQQFu3bqFmJgYKBQKHDx4EBkZGfj000+RmpoKANixYweysrKwZ88eREZGYuvWrTh//jwuXrx4zy/h7uYMcSEi6stlnQFP7i7C4zPDkPnY5F7zgNUqcElnwPnrTTC0m5Ewzh8zwtV4fm8pDp2vhcrDDQE+SowP8Mbz86IRF65GVUMrdh/vmpIiOtAbKQ8FwcfTA2tyTvdr1FUACFWrcOJnP3yguWnIbl1766238Oqrr+Lo0aNISEjo71MCYEIhouFhsPMiCCHQ0GKCv5cC1fo2HLlYh/GB3pgzQQOZDPjb59X46Mw1WIVAoK8Sj4zX4FaLCWU1BlTr2yCTAd5Kd1ysNaDdbEFcuBpfXGtCm9kiPUfp1kcHNbCCM7z/2ts3FABmzpyJhQsX4pVXXoEQAiEhIcjIyMDmzZsBdN0xEBQUhJycHDz11FN9Hs8Z4kJE1B/2zNkz0H0vVDfhF59cwBfX9Aj288SmRyeiqrEVnh5uSIr0h4+nB85XN+GT0mpMD/fDi6kxg2rXA5kwVKFQID4+Hnl5eVKhY7VakZeXhw0bNtxzv507d+K1117D4cOHB1zkEBENF4NNJDKZTOqDE+7vhZ/MjrRZvyw+DMviw/o8jhACQgByuQz6VhPOXdNjzCgltH6eUHsNr6FDu5lMJpSUlGDLli3SMrlcjpSUFBQVFfW5vxAC+fn5KC8vx44dOwAAlZWV0Ol0SElJkbbz8/NDUlISioqK+lXoEBENF/ZMTDrQfaeG+mH/ukdw9ttbiAn26XXY6klaHyyLD8NQTOU5oEIHADZt2oTVq1cjISEBiYmJePvtt9HS0oI1a9YAAFatWoXQ0FBkZWUB6Lo9YNu2bXj//fcxbtw46HQ6AIC3tze8vTk7OBHR/SKTydCdk9ReCsyd1PO2ruHm5s2bsFgsCAoKslkeFBSEy5cv33O/pqYmhIaGoqOjA25ubvjd736HRx99FACkPNTbMbvX/avebqkmIqKe5HIZEiP7nujUngKsvwZc6Cxfvhw3btzAtm3boNPpEBcXh9zcXClhVFVVQS6/0+k1OzsbJpMJy5YtszlOZmYmtm/fbl/riYiIeuHj44Nz586hubkZeXl52LRpE6KiojB37txBHS8rK6vXQXKIiMh5DbjQAYANGzbc81a1goICm7+vXr06mKcgIiKCRqOBm5sb6urqbJbX1dVBq733vENyuRwTJkwAAMTFxeHSpUvIysrC3Llzpf3q6uoQHBxsc8y4uLhej7dlyxZs2rRJ+rt7kBwiInJegxtvlIiIaAjc3Te0W3ff0LtH/+yL1WqVbj2LjIyEVqu1OabBYEBxcfE9j6lUKuHr62vzICIi5zaoX3SIiIiGykD7hmZlZSEhIQHjx49HR0cHDh06hD//+c/Izs4G0HVf+MaNG/Hqq68iOjpaGl46JCTEZkRRIiIa3ljoEBGRUxto39CWlhasX78e169fh0qlQkxMDP7yl79g+fLl0jYvvfQSWlpasHbtWuj1esyePRu5ubn9mkOHiIiGhwHNo+MonK+AiMgx+P7bO8aFiMhx+vsezD46RERERETkcljoEBERERGRy2GhQ0RERERELoeFDhERERERuZxhMepa93gJBoPBwS0hIhpZut93h8G4NUOKeYmIyHH6m5uGRaFjNBoBgLNQExE5iNFohJ+fn6Ob4TSYl4iIHK+v3DQshpe2Wq2oqamBj48PZDLZgPc3GAwIDw/HtWvXOAzoIDB+9mMM7cP42W+wMRRCwGg0IiQkxGaumpGOecnxGEP7MH72YwztY0/8+pubhsUvOnK5HGFhYXYfx9fXly9EOzB+9mMM7cP42W8wMeQvOT0xLzkPxtA+jJ/9GEP7DDZ+/clN/HqOiIiIiIhcDgsdIiIiIiJyOSOi0FEqlcjMzIRSqXR0U4Ylxs9+jKF9GD/7MYbOhf8f9mMM7cP42Y8xtM9QxG9YDEZAREREREQ0ECPiFx0iIiIiIhpZWOgQEREREZHLYaFDREREREQuh4UOERERERG5HJcvdHbt2oVx48bB09MTSUlJOH36tKOb5LS2b98OmUxm84iJiZHWt7e3Iz09HWPGjIG3tzeeeOIJ1NXVObDFjnX8+HE89thjCAkJgUwmwyeffGKzXgiBbdu2ITg4GCqVCikpKbhy5YrNNo2NjVi5ciV8fX2hVqvxzDPPoLm5eQjPwrH6iuGPf/zjHq/JtLQ0m21GagyzsrLw8MMPw8fHB4GBgViyZAnKy8tttunPNVtVVYWFCxfCy8sLgYGBePHFF9HZ2TmUpzIiMTf1D/PSwDE32Yd5yT7OlptcutD58MMPsWnTJmRmZuLzzz/H9OnTkZqaivr6ekc3zWlNmTIFtbW10uPEiRPSuhdeeAF///vfsW/fPhQWFqKmpgaPP/64A1vrWC0tLZg+fTp27drV6/qdO3finXfewe7du1FcXIxRo0YhNTUV7e3t0jYrV65EWVkZjhw5goMHD+L48eNYu3btUJ2Cw/UVQwBIS0uzeU3u3bvXZv1IjWFhYSHS09Nx6tQpHDlyBGazGfPnz0dLS4u0TV/XrMViwcKFC2EymXDy5Ens2bMHOTk52LZtmyNOacRgbhoY5qWBYW6yD/OSfZwuNwkXlpiYKNLT06W/LRaLCAkJEVlZWQ5slfPKzMwU06dP73WdXq8XHh4eYt++fdKyS5cuCQCiqKhoiFrovACI/fv3S39brVah1WrFm2++KS3T6/VCqVSKvXv3CiGEuHjxogAgzpw5I23zj3/8Q8hkMlFdXT1kbXcW/xpDIYRYvXq1WLx48T33YQzvqK+vFwBEYWGhEKJ/1+yhQ4eEXC4XOp1O2iY7O1v4+vqKjo6OoT2BEYS5qf+Yl+zD3GQf5iX7OTo3uewvOiaTCSUlJUhJSZGWyeVypKSkoKioyIEtc25XrlxBSEgIoqKisHLlSlRVVQEASkpKYDabbeIZExODiIgIxrMXlZWV0Ol0NvHy8/NDUlKSFK+ioiKo1WokJCRI26SkpEAul6O4uHjI2+ysCgoKEBgYiEmTJmHdunVoaGiQ1jGGdzQ1NQEA/P39AfTvmi0qKsK0adMQFBQkbZOamgqDwYCysrIhbP3Iwdw0cMxL9w9z0/3BvNR/js5NLlvo3Lx5ExaLxSZIABAUFASdTuegVjm3pKQk5OTkIDc3F9nZ2aisrMScOXNgNBqh0+mgUCigVqtt9mE8e9cdk+96/el0OgQGBtqsd3d3h7+/P2N6W1paGv70pz8hLy8PO3bsQGFhIRYsWACLxQKAMexmtVqxceNGzJo1C1OnTgWAfl2zOp2u19do9zq6/5ibBoZ56f5ibrIf81L/OUNuch9k28kFLViwQPp3bGwskpKSMHbsWHz00UdQqVQObBmNVE899ZT072nTpiE2Nhbjx49HQUEB5s2b58CWOZf09HRcuHDBpu8CkStgXiJnw7zUf86Qm1z2Fx2NRgM3N7ceozjU1dVBq9U6qFXDi1qtxsSJE1FRUQGtVguTyQS9Xm+zDePZu+6YfNfrT6vV9uh83NnZicbGRsb0HqKioqDRaFBRUQGAMQSADRs24ODBgzh27BjCwsKk5f25ZrVaba+v0e51dP8xN9mHeck+zE33H/NS75wlN7lsoaNQKBAfH4+8vDxpmdVqRV5eHpKTkx3YsuGjubkZX3/9NYKDgxEfHw8PDw+beJaXl6Oqqorx7EVkZCS0Wq1NvAwGA4qLi6V4JScnQ6/Xo6SkRNomPz8fVqsVSUlJQ97m4eD69etoaGhAcHAwgJEdQyEENmzYgP379yM/Px+RkZE26/tzzSYnJ+P8+fM2SfnIkSPw9fXF5MmTh+ZERhjmJvswL9mHuen+Y16y5XS5ye7hFJzYBx98IJRKpcjJyREXL14Ua9euFWq12mYUB7ojIyNDFBQUiMrKSvHZZ5+JlJQUodFoRH19vRBCiOeee05ERESI/Px8cfbsWZGcnCySk5Md3GrHMRqNorS0VJSWlgoA4te//rUoLS0V3377rRBCiDfeeEOo1Wpx4MAB8eWXX4rFixeLyMhI0dbWJh0jLS1NzJgxQxQXF4sTJ06I6OhosWLFCked0pD7rhgajUaxefNmUVRUJCorK8XRo0fFzJkzRXR0tGhvb5eOMVJjuG7dOuHn5ycKCgpEbW2t9GhtbZW26eua7ezsFFOnThXz588X586dE7m5uSIgIEBs2bLFEac0YjA39R/z0sAxN9mHeck+zpabXLrQEUKI3/72tyIiIkIoFAqRmJgoTp065egmOa3ly5eL4OBgoVAoRGhoqFi+fLmoqKiQ1re1tYn169eL0aNHCy8vL7F06VJRW1vrwBY71rFjxwSAHo/Vq1cLIbqG8dy6dasICgoSSqVSzJs3T5SXl9sco6GhQaxYsUJ4e3sLX19fsWbNGmE0Gh1wNo7xXTFsbW0V8+fPFwEBAcLDw0OMHTtWPPvssz0+DI7UGPYWNwDij3/8o7RNf67Zq1evigULFgiVSiU0Go3IyMgQZrN5iM9m5GFu6h/mpYFjbrIP85J9nC03yW43ioiIiIiIyGW4bB8dIiIiIiIauVjoEBERERGRy2GhQ0RERERELoeFDhERERERuRwWOkRERERE5HJY6BARERERkcthoUNERERERC6HhQ4REREREbkcFjpERERERORyWOgQEREREZHLYaFDREREREQuh4UOERERERG5nP8H2sIrtgZ/TW8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "RMSE: 0.5168807257879506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS PRAKTIKUM\n",
        "\n",
        "Gunakan JST untuk klasifikasi angka tulisan tangan (MNIST).\n",
        "\n"
      ],
      "metadata": {
        "id": "hAKP1bETjdAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 → 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Bangun model JST\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)), # Ubah gambar 28x28 menjadi vektor\n",
        "    Dense(128, activation='relu'), # Hidden layer 1\n",
        "    Dense(64, activation='relu'),  # Hidden layer 2\n",
        "    Dense(10, activation='softmax') # Output layer (10 kelas)\n",
        "])\n",
        "\n",
        "# 3. Kompilasi model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Latih model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# 5. Evaluasi model\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oJt-gvVijeRZ",
        "outputId": "350a2cbf-9a01-4238-a993-b440afef76b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.8744 - loss: 0.4270 - val_accuracy: 0.9661 - val_loss: 0.1114\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9676 - loss: 0.1067 - val_accuracy: 0.9735 - val_loss: 0.0854\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9773 - loss: 0.0709 - val_accuracy: 0.9724 - val_loss: 0.0856\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0522 - val_accuracy: 0.9762 - val_loss: 0.0793\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9874 - loss: 0.0392 - val_accuracy: 0.9759 - val_loss: 0.0808\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9902 - loss: 0.0307 - val_accuracy: 0.9777 - val_loss: 0.0779\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9915 - loss: 0.0260 - val_accuracy: 0.9748 - val_loss: 0.0909\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.0205 - val_accuracy: 0.9794 - val_loss: 0.0812\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9942 - loss: 0.0173 - val_accuracy: 0.9770 - val_loss: 0.0972\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9941 - loss: 0.0174 - val_accuracy: 0.9780 - val_loss: 0.0930\n",
            "Akurasi pada data uji: 0.9780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coba dengan beberapa parameter lain:"
      ],
      "metadata": {
        "id": "4UhIAHV4zZUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ubah jumlah neuron di hidden layer (misal: 256 dan 128).\n"
      ],
      "metadata": {
        "id": "FwFhdBVwzpJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 → 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Model JST\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. Kompilasi\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Early Stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',   # berhenti jika val_loss tidak membaik\n",
        "    patience=10,           # toleransi 10 epoch\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 5. Latih model (banyak epoch)\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,             # epoch lebih besar\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# 6. Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu--zO7mznVC",
        "outputId": "26073f02-603c-47f4-9ec3-85328fa4035e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.8928 - loss: 0.3595 - val_accuracy: 0.9616 - val_loss: 0.1274\n",
            "Epoch 2/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9742 - loss: 0.0837 - val_accuracy: 0.9737 - val_loss: 0.0876\n",
            "Epoch 3/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9816 - loss: 0.0579 - val_accuracy: 0.9777 - val_loss: 0.0732\n",
            "Epoch 4/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9866 - loss: 0.0419 - val_accuracy: 0.9759 - val_loss: 0.0820\n",
            "Epoch 5/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9908 - loss: 0.0296 - val_accuracy: 0.9814 - val_loss: 0.0697\n",
            "Epoch 6/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9913 - loss: 0.0262 - val_accuracy: 0.9742 - val_loss: 0.1004\n",
            "Epoch 7/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9928 - loss: 0.0220 - val_accuracy: 0.9770 - val_loss: 0.0882\n",
            "Epoch 8/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9939 - loss: 0.0184 - val_accuracy: 0.9815 - val_loss: 0.0735\n",
            "Epoch 9/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9955 - loss: 0.0134 - val_accuracy: 0.9787 - val_loss: 0.0942\n",
            "Epoch 10/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9952 - loss: 0.0148 - val_accuracy: 0.9784 - val_loss: 0.0942\n",
            "Epoch 11/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9956 - loss: 0.0134 - val_accuracy: 0.9778 - val_loss: 0.0979\n",
            "Epoch 12/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9965 - loss: 0.0106 - val_accuracy: 0.9776 - val_loss: 0.0920\n",
            "Epoch 13/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9959 - loss: 0.0132 - val_accuracy: 0.9805 - val_loss: 0.0991\n",
            "Epoch 14/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9963 - loss: 0.0110 - val_accuracy: 0.9802 - val_loss: 0.1025\n",
            "Epoch 15/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9964 - loss: 0.0113 - val_accuracy: 0.9806 - val_loss: 0.1045\n",
            "Akurasi pada data uji: 0.9814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tambahkan satu hidden layer lagi.\n"
      ],
      "metadata": {
        "id": "F4JN3xsxzxi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 → 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Model JST\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)), # flatten image dari 28x28 menjadi vektor\n",
        "    Dense(256, activation='relu'), # Hidden layer 1\n",
        "    Dense(128, activation='relu'), # Hidden layer 2\n",
        "    Dense(64, activation='relu'), # Hidden layer 3\n",
        "    Dense(10, activation='softmax') # Output layer (10 kelas)\n",
        "])\n",
        "\n",
        "# 3. Kompilasi\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Early Stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',   # berhenti jika val_loss tidak membaik\n",
        "    patience=10,           # toleransi 10 epoch\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 5. Latih model (banyak epoch)\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,             # epoch lebih besar\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# 6. Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7Nh6YQHzzAl",
        "outputId": "68f99813-f395-4386-9697-96595ecbc338"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.8886 - loss: 0.3719 - val_accuracy: 0.9648 - val_loss: 0.1131\n",
            "Epoch 2/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9735 - loss: 0.0888 - val_accuracy: 0.9748 - val_loss: 0.0834\n",
            "Epoch 3/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9811 - loss: 0.0602 - val_accuracy: 0.9768 - val_loss: 0.0817\n",
            "Epoch 4/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9849 - loss: 0.0471 - val_accuracy: 0.9784 - val_loss: 0.0732\n",
            "Epoch 5/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9874 - loss: 0.0382 - val_accuracy: 0.9758 - val_loss: 0.0864\n",
            "Epoch 6/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9887 - loss: 0.0342 - val_accuracy: 0.9741 - val_loss: 0.0950\n",
            "Epoch 7/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9920 - loss: 0.0235 - val_accuracy: 0.9763 - val_loss: 0.0918\n",
            "Epoch 8/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.9931 - loss: 0.0210 - val_accuracy: 0.9786 - val_loss: 0.0846\n",
            "Epoch 9/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9934 - loss: 0.0206 - val_accuracy: 0.9749 - val_loss: 0.1051\n",
            "Epoch 10/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9939 - loss: 0.0180 - val_accuracy: 0.9807 - val_loss: 0.0869\n",
            "Epoch 11/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9944 - loss: 0.0179 - val_accuracy: 0.9759 - val_loss: 0.1073\n",
            "Epoch 12/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9955 - loss: 0.0146 - val_accuracy: 0.9815 - val_loss: 0.0832\n",
            "Epoch 13/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9966 - loss: 0.0126 - val_accuracy: 0.9795 - val_loss: 0.0996\n",
            "Epoch 14/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9952 - loss: 0.0157 - val_accuracy: 0.9826 - val_loss: 0.0890\n",
            "Akurasi pada data uji: 0.9784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bandingkan akurasi dan waktu pelatihan.\n",
        "\n",
        "Perbandingan Dua Model JST MNIST\n",
        "\n",
        "| Aspek | Model 1 (256→128→Softmax) | Model 2 (256→128→64→Softmax) |\n",
        "|------|---------------------------|-------------------------------|\n",
        "| Jumlah Hidden Layer | 2 | 3 |\n",
        "| Val Loss Terbaik | 0.0697 | 0.0732 |\n",
        "| Val Accuracy Terbaik | 0.9815 | 0.9826 |\n",
        "| Akurasi Akhir Test | **0.9814** | **0.9784** |\n",
        "| Rata-rata Waktu per Epoch | ~12.5 detik | ~13.5 detik |\n",
        "| Total Epoch (Early Stop) | 15 | 14 |\n",
        "| Total Waktu Komputasi | ±187 detik (3 menit 7 detik) | ±189 detik (3 menit 9 detik) |\n",
        "| Efisiensi Latihan | Lebih efisien | Sedikit lebih lambat |\n",
        "| Risiko Overfitting | Lebih rendah | Lebih tinggi |\n",
        "\n",
        "\n",
        "\n",
        "Model kedua memiliki arsitektur yang lebih dalam, sehingga jumlah parameternya lebih banyak. Untuk dataset sederhana seperti MNIST, kapasitas model yang terlalu besar menyebabkan overfitting lebih cepat, di mana model belajar pola yang tidak relevan dan kehilangan kemampuan generalisasi pada data uji. Selain itu, penambahan layer tambahan tidak memberikan informasi baru yang signifikan bagi MLP untuk gambar sederhana 28×28, sehingga justru membuat proses pelatihan lebih tidak stabil dan membutuhkan waktu komputasi lebih lama.\n"
      ],
      "metadata": {
        "id": "kZegx1PQz5V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eksperimen dengan fungsi aktivasi Sigmoid vs ReLU.\n",
        "\n",
        "Model sigmoid masih menghasilkan akurasi tinggi, namun tetap sedikit di bawah ReLU karena sigmoid lebih rentan mengalami vanishing gradient, sehingga proses belajar lebih lambat dan kadang berhenti di titik suboptimal. Secara waktu, sigmoid juga lebih lambat karena fungsi aktivasi sigmoid membutuhkan komputasi eksponensial.\n",
        "\n",
        " Perbandingan Model ReLU vs Sigmoid (Arsitektur Sama)\n",
        "\n",
        "| Aspek | Model ReLU (256→128) | Model Sigmoid (256→128) |\n",
        "|------|-----------------------|---------------------------|\n",
        "| Aktivasi Hidden Layer | ReLU | Sigmoid |\n",
        "| Epoch Total (Early Stop) | 15 | 19 |\n",
        "| Val Loss Terbaik | **0.0697** | **0.0680** |\n",
        "| Val Accuracy Terbaik | **0.9815** | **0.9816** |\n",
        "| Akurasi Akhir Test | **0.9814** | **0.9812** |\n",
        "| Rata-rata Waktu per Epoch | ~12.5 detik | ~13.5 detik |\n",
        "| Total Waktu Pelatihan | ±187 detik | ±256 detik |\n",
        "| Kecepatan Konvergensi | Lebih cepat | Lebih lambat |\n",
        "| Risiko Vanishing Gradient | Rendah | Lebih tinggi |\n",
        "| Kompleksitas Komputasi | Ringan | Lebih berat (fungsi sigmoid) |\n",
        "\n"
      ],
      "metadata": {
        "id": "h_d1A76rz7oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 → 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Model JST\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)), # flatten image dari 28x28 menjadi vektor\n",
        "    Dense(256, activation='sigmoid'), # Hidden layer 1\n",
        "    Dense(128, activation='sigmoid'), # Hidden layer 2\n",
        "    Dense(10, activation='softmax') # Output layer (10 kelas)\n",
        "])\n",
        "\n",
        "# 3. Kompilasi\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Early Stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',   # berhenti jika val_loss tidak membaik\n",
        "    patience=10,           # toleransi 10 epoch\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 5. Latih model (banyak epoch)\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,             # epoch lebih besar\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# 6. Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyBHcNsvz7D-",
        "outputId": "e34ab8da-618b-46a4-bf4d-e171cf1e97b8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.8054 - loss: 0.7042 - val_accuracy: 0.9432 - val_loss: 0.1885\n",
            "Epoch 2/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9494 - loss: 0.1700 - val_accuracy: 0.9594 - val_loss: 0.1329\n",
            "Epoch 3/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9672 - loss: 0.1102 - val_accuracy: 0.9665 - val_loss: 0.1012\n",
            "Epoch 4/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9773 - loss: 0.0750 - val_accuracy: 0.9736 - val_loss: 0.0830\n",
            "Epoch 5/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9837 - loss: 0.0575 - val_accuracy: 0.9765 - val_loss: 0.0733\n",
            "Epoch 6/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9876 - loss: 0.0414 - val_accuracy: 0.9769 - val_loss: 0.0750\n",
            "Epoch 7/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9903 - loss: 0.0314 - val_accuracy: 0.9769 - val_loss: 0.0756\n",
            "Epoch 8/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9937 - loss: 0.0219 - val_accuracy: 0.9797 - val_loss: 0.0696\n",
            "Epoch 9/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.0169 - val_accuracy: 0.9812 - val_loss: 0.0680\n",
            "Epoch 10/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9965 - loss: 0.0123 - val_accuracy: 0.9757 - val_loss: 0.0816\n",
            "Epoch 11/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9965 - loss: 0.0116 - val_accuracy: 0.9802 - val_loss: 0.0704\n",
            "Epoch 12/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9976 - loss: 0.0080 - val_accuracy: 0.9802 - val_loss: 0.0765\n",
            "Epoch 13/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9986 - loss: 0.0059 - val_accuracy: 0.9811 - val_loss: 0.0797\n",
            "Epoch 14/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 0.9799 - val_loss: 0.0817\n",
            "Epoch 15/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 0.9806 - val_loss: 0.0841\n",
            "Epoch 16/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 0.9816 - val_loss: 0.0800\n",
            "Epoch 17/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0024 - val_accuracy: 0.9801 - val_loss: 0.0869\n",
            "Epoch 18/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 0.9709 - val_loss: 0.1340\n",
            "Epoch 19/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 0.9812 - val_loss: 0.0899\n",
            "Akurasi pada data uji: 0.9812\n"
          ]
        }
      ]
    }
  ]
}